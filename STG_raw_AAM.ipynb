{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "'''\n",
    "Developed by Abdullah Al Mamun\n",
    "- Feature selection framework \n",
    "- Date: Feb 2019\n",
    "'''\n",
    "\n",
    "# Load pkgses\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# Masrur confusion matrix\n",
    "import collections\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pylab import savefig\n",
    "\n",
    "# STG comment if local\n",
    "# from stg import STG\n",
    "# import scipy.stats # for creating a simple dataset \n",
    "# import torch\n",
    "\n",
    "#keep comment otherwise fig will be distorted\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "# import seaborn as sns; sns.set(color_codes=True)\n",
    "# from pylab import savefig\n",
    "\n",
    "\n",
    "PATH = ''\n",
    "fileName = 'Example-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000005206.15</th>\n",
       "      <th>ENSG00000083622.8</th>\n",
       "      <th>ENSG00000088970.14</th>\n",
       "      <th>ENSG00000099869.7</th>\n",
       "      <th>ENSG00000100181.20</th>\n",
       "      <th>ENSG00000104691.13</th>\n",
       "      <th>ENSG00000115934.11</th>\n",
       "      <th>ENSG00000117242.7</th>\n",
       "      <th>ENSG00000118412.11</th>\n",
       "      <th>ENSG00000122043.9</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000225138.6</th>\n",
       "      <th>ENSG00000225140.1</th>\n",
       "      <th>ENSG00000225144.2</th>\n",
       "      <th>ENSG00000225146.1</th>\n",
       "      <th>ENSG00000225148.1</th>\n",
       "      <th>ENSG00000225152.1</th>\n",
       "      <th>ENSG00000225156.2</th>\n",
       "      <th>ENSG00000225163.3</th>\n",
       "      <th>ENSG00000225166.1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.474815</td>\n",
       "      <td>1.015497</td>\n",
       "      <td>3.938578</td>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.217501</td>\n",
       "      <td>2.445204</td>\n",
       "      <td>0.024322</td>\n",
       "      <td>1.651243</td>\n",
       "      <td>1.495478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.775220</td>\n",
       "      <td>0.038819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14878</td>\n",
       "      <td>0.01209</td>\n",
       "      <td>0.265130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.313495</td>\n",
       "      <td>0.608207</td>\n",
       "      <td>2.179453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015612</td>\n",
       "      <td>1.441505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938922</td>\n",
       "      <td>1.537811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.017635</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07404</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.257730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.396907</td>\n",
       "      <td>1.475637</td>\n",
       "      <td>2.591849</td>\n",
       "      <td>0.016740</td>\n",
       "      <td>0.023794</td>\n",
       "      <td>2.690200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.583583</td>\n",
       "      <td>1.415676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.744041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.377362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.960528</td>\n",
       "      <td>1.142912</td>\n",
       "      <td>2.312048</td>\n",
       "      <td>0.010347</td>\n",
       "      <td>0.043719</td>\n",
       "      <td>2.203999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965548</td>\n",
       "      <td>1.920312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.571482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.280320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.296875</td>\n",
       "      <td>0.447136</td>\n",
       "      <td>2.104808</td>\n",
       "      <td>0.162264</td>\n",
       "      <td>0.178781</td>\n",
       "      <td>1.747109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762944</td>\n",
       "      <td>1.715874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.415390</td>\n",
       "      <td>0.132493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.686951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000005206.15  ENSG00000083622.8  ENSG00000088970.14  \\\n",
       "0            4.474815           1.015497            3.938578   \n",
       "1            4.313495           0.608207            2.179453   \n",
       "2            4.396907           1.475637            2.591849   \n",
       "3            3.960528           1.142912            2.312048   \n",
       "4            4.296875           0.447136            2.104808   \n",
       "\n",
       "   ENSG00000099869.7  ENSG00000100181.20  ENSG00000104691.13  \\\n",
       "0           0.011318            0.217501            2.445204   \n",
       "1           0.000000            0.015612            1.441505   \n",
       "2           0.016740            0.023794            2.690200   \n",
       "3           0.010347            0.043719            2.203999   \n",
       "4           0.162264            0.178781            1.747109   \n",
       "\n",
       "   ENSG00000115934.11  ENSG00000117242.7  ENSG00000118412.11  \\\n",
       "0            0.024322           1.651243            1.495478   \n",
       "1            0.000000           0.938922            1.537811   \n",
       "2            0.000000           1.583583            1.415676   \n",
       "3            0.000000           0.965548            1.920312   \n",
       "4            0.000000           0.762944            1.715874   \n",
       "\n",
       "   ENSG00000122043.9  ...  ENSG00000225138.6  ENSG00000225140.1  \\\n",
       "0                0.0  ...           2.775220           0.038819   \n",
       "1                0.0  ...           5.017635           0.018949   \n",
       "2                0.0  ...           3.744041           0.000000   \n",
       "3                0.0  ...           4.571482           0.000000   \n",
       "4                0.0  ...           2.415390           0.132493   \n",
       "\n",
       "   ENSG00000225144.2  ENSG00000225146.1  ENSG00000225148.1  ENSG00000225152.1  \\\n",
       "0                0.0           0.059680                0.0            0.14878   \n",
       "1                0.0           0.057897                0.0            0.07404   \n",
       "2                0.0           0.000000                0.0            0.00000   \n",
       "3                0.0           0.107283                0.0            0.00000   \n",
       "4                0.0           0.117850                0.0            0.00000   \n",
       "\n",
       "   ENSG00000225156.2  ENSG00000225163.3  ENSG00000225166.1  target  \n",
       "0            0.01209           0.265130                0.0       0  \n",
       "1            0.00000           0.257730                0.0       0  \n",
       "2            0.00000           0.377362                0.0       0  \n",
       "3            0.00000           1.280320                0.0       0  \n",
       "4            0.00000           0.686951                0.0       0  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "#     loading data, minMax normalization and train tesing split\n",
    "df = pd.read_csv(PATH + fileName)\n",
    "df.fillna(0.0)\n",
    "# df = df.replace(np.nan,0.0)\n",
    "X = df.iloc[:,1:-1]\n",
    "y = df.iloc[:,-1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 1022)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "cls = y.unique()\n",
    "len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 1022) (40, 1022)\n",
      "(127, 1022) (32, 1022)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify = y, random_state = 1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8, stratify = y_train, random_state = 1)\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. utils.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import six\n",
    "import collections\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "\n",
    "SKIP_TYPES = six.string_types\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    '''\n",
    "    Assuming X and y are numpy arrays and \n",
    "     with X.shape = (n_samples, n_features) \n",
    "          y.shape = (n_samples,)\n",
    "    '''\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.X[i]\n",
    "        data = np.array(data).astype(np.float32)\n",
    "        if self.y is not None:\n",
    "            return dict(input=data, label=self.y[i])\n",
    "        else:\n",
    "            return dict(input=data)\n",
    "\n",
    "\n",
    "class FastTensorDataLoader:\n",
    "    \"\"\"\n",
    "    A DataLoader-like object for a set of tensors that can be much faster than\n",
    "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
    "    the dataset and calls cat (slow).\n",
    "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
    "    \"\"\"\n",
    "    def __init__(self, *tensors, tensor_names, batch_size=32, shuffle=False):\n",
    "        \"\"\"\n",
    "        Initialize a FastTensorDataLoader.\n",
    "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
    "        :param tensor_names: name of tensors (for feed_dict)\n",
    "        :param batch_size: batch size to load.\n",
    "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
    "            iterator is created out of this object.\n",
    "        :returns: A FastTensorDataLoader.\n",
    "        \"\"\"\n",
    "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.tensor_names = tensor_names\n",
    "\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Calculate # batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder > 0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            r = torch.randperm(self.dataset_len)\n",
    "            self.tensors = [t[r] for t in self.tensors]\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        batch = {}\n",
    "        for k in range(len(self.tensor_names)):\n",
    "            batch.update({self.tensor_names[k]: self.tensors[k][self.i:self.i+self.batch_size]})\n",
    "        self.i += self.batch_size\n",
    "        return batch\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "\n",
    "'''standardize_dataset function is from utils_jared.py'''\n",
    "def standardize_dataset(dataset, offset, scale):\n",
    "    norm_ds = copy.deepcopy(dataset)\n",
    "    norm_ds['x'] = (norm_ds['x'] - offset) / scale\n",
    "    return norm_ds\n",
    "\n",
    "\n",
    "'''load_datasets function is from utils_jared.py'''\n",
    "def load_datasets(dataset_file):\n",
    "    datasets = defaultdict(dict)\n",
    "    with h5py.File(dataset_file, 'r') as fp:\n",
    "        for ds in fp:\n",
    "            for array in fp[ds]:\n",
    "                datasets[ds][array] = fp[ds][array][:]\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def load_cox_gaussian_data():\n",
    "    dataset_file = os.path.join(os.path.dirname(__file__), \n",
    "        'datasets/gaussian_survival_data.h5')\n",
    "    datasets = defaultdict(dict)\n",
    "    with h5py.File(dataset_file, 'r') as fp:\n",
    "        for ds in fp:\n",
    "            for array in fp[ds]:\n",
    "                datasets[ds][array] = fp[ds][array][:]\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def prepare_data(x, label):\n",
    "    if isinstance(label, dict):\n",
    "       e, t = label['e'], label['t']\n",
    "\n",
    "    # Sort training data for accurate partial likelihood calculation.\n",
    "    sort_idx = np.argsort(t)[::-1]\n",
    "    x = x[sort_idx]\n",
    "    e = e[sort_idx]\n",
    "    t = t[sort_idx]\n",
    "\n",
    "    #return x, {'e': e, 't': t} this is for parse_data(x, label); see the third line in the parse_data function. \n",
    "    return {'x': x, 'e': e, 't': t}\n",
    "\n",
    "def probe_infnan(v, name, extras={}):\n",
    "    nps = torch.isnan(v)\n",
    "    s = nps.sum().item()\n",
    "    if s > 0:\n",
    "        print('>>> {} >>>'.format(name))\n",
    "        print(name, s)\n",
    "        print(v[nps])\n",
    "        for k, val in extras.items():\n",
    "            print(k, val, val.sum().item())\n",
    "        quit()\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, *args):\n",
    "        if len(args) == 1:\n",
    "            return args[0]\n",
    "        return args\n",
    "\n",
    "def get_batcnnorm(bn, nr_features=None, nr_dims=1):\n",
    "    if isinstance(bn, nn.Module):\n",
    "        return bn\n",
    "\n",
    "    assert 1 <= nr_dims <= 3\n",
    "\n",
    "    if bn in (True, 'async'):\n",
    "        clz_name = 'BatchNorm{}d'.format(nr_dims)\n",
    "        return getattr(nn, clz_name)(nr_features)\n",
    "    else:\n",
    "        raise ValueError('Unknown type of batch normalization: {}.'.format(bn))\n",
    "\n",
    "\n",
    "def get_dropout(dropout, nr_dims=1):\n",
    "    if isinstance(dropout, nn.Module):\n",
    "        return dropout\n",
    "\n",
    "    if dropout is True:\n",
    "        dropout = 0.5\n",
    "    if nr_dims == 1:\n",
    "        return nn.Dropout(dropout, True)\n",
    "    else:\n",
    "        clz_name = 'Dropout{}d'.format(nr_dims)\n",
    "        return getattr(nn, clz_name)(dropout)\n",
    "\n",
    "\n",
    "def get_activation(act):\n",
    "    if isinstance(act, nn.Module):\n",
    "        return act\n",
    "\n",
    "    assert type(act) is str, 'Unknown type of activation: {}.'.format(act)\n",
    "    act_lower = act.lower()\n",
    "    if act_lower == 'identity':\n",
    "        return Identity()\n",
    "    elif act_lower == 'relu':\n",
    "        return nn.ReLU(True)\n",
    "    elif act_lower == 'selu':\n",
    "        return nn.SELU(True)\n",
    "    elif act_lower == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif act_lower == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        try:\n",
    "            return getattr(nn, act)\n",
    "        except AttributeError:\n",
    "            raise ValueError('Unknown activation function: {}.'.format(act))\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer, model, *args, **kwargs):\n",
    "    if isinstance(optimizer, (optim.Optimizer)):\n",
    "        return optimizer\n",
    "\n",
    "    if type(optimizer) is str:\n",
    "        try:\n",
    "            optimizer = getattr(optim, optimizer)\n",
    "        except AttributeError:\n",
    "            raise ValueError('Unknown optimizer type: {}.'.format(optimizer))\n",
    "    return optimizer(filter(lambda p: p.requires_grad, model.parameters()), *args, **kwargs)\n",
    "    \n",
    "\n",
    "def stmap(func, iterable):\n",
    "    if isinstance(iterable, six.string_types):\n",
    "        return func(iterable)\n",
    "    elif isinstance(iterable, (collections.Sequence, collections.UserList)):\n",
    "        return [stmap(func, v) for v in iterable]\n",
    "    elif isinstance(iterable, collections.Set):\n",
    "        return {stmap(func, v) for v in iterable}\n",
    "    elif isinstance(iterable, (collections.Mapping, collections.UserDict)):\n",
    "        return {k: stmap(func, v) for k, v in iterable.items()}\n",
    "    else:\n",
    "        return func(iterable)\n",
    "\n",
    "\n",
    "def _as_tensor(o):\n",
    "    from torch.autograd import Variable\n",
    "    if isinstance(o, SKIP_TYPES):\n",
    "        return o\n",
    "    if isinstance(o, Variable):\n",
    "        return o\n",
    "    if torch.is_tensor(o):\n",
    "        return o\n",
    "    return torch.from_numpy(np.array(o))\n",
    "\n",
    "\n",
    "def as_tensor(obj):\n",
    "    return stmap(_as_tensor, obj)\n",
    "\n",
    "\n",
    "def _as_numpy(o):\n",
    "    from torch.autograd import Variable\n",
    "    if isinstance(o, SKIP_TYPES):\n",
    "        return o\n",
    "    if isinstance(o, Variable):\n",
    "        o = o\n",
    "    if torch.is_tensor(o):\n",
    "        return o.cpu().numpy()\n",
    "    return np.array(o)\n",
    "\n",
    "\n",
    "def as_numpy(obj):\n",
    "    return stmap(_as_numpy, obj)\n",
    "\n",
    "\n",
    "def _as_float(o):\n",
    "    if isinstance(o, SKIP_TYPES):\n",
    "        return o\n",
    "    if torch.is_tensor(o):\n",
    "        return o.item()\n",
    "    arr = as_numpy(o)\n",
    "    assert arr.size == 1\n",
    "    return float(arr)\n",
    "\n",
    "\n",
    "def as_float(obj):\n",
    "    return stmap(_as_float, obj)\n",
    "\n",
    "\n",
    "def _as_cpu(o):\n",
    "    from torch.autograd import Variable\n",
    "    if isinstance(o, Variable) or torch.is_tensor(o):\n",
    "        return o.cpu()\n",
    "    return o\n",
    "\n",
    "\n",
    "def as_cpu(obj):\n",
    "    return stmap(_as_cpu, obj)\n",
    "\n",
    "\n",
    "## For synthetic dataset creation\n",
    "import math\n",
    "from sklearn.datasets import make_moons\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# Create a simple dataset\n",
    "def create_twomoon_dataset(n, p):\n",
    "    relevant, y = make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)\n",
    "    print(y.shape)\n",
    "    noise_vector = norm.rvs(loc=0, scale=1, size=[n,p-2])\n",
    "    data = np.concatenate([relevant, noise_vector], axis=1)\n",
    "    print(data.shape)\n",
    "    return data, y\n",
    "\n",
    "\n",
    "def create_sin_dataset(n,p):\n",
    "    x1=5*(np.random.uniform(0,1,n)).reshape(-1,1)\n",
    "    x2=5*(np.random.uniform(0,1,n)).reshape(-1,1)\n",
    "    y=np.sin(x1)*np.cos(x2)**3\n",
    "    relevant=np.hstack((x1,x2))\n",
    "    noise_vector = norm.rvs(loc=0, scale=1, size=[n,p-2])\n",
    "    data = np.concatenate([relevant, noise_vector], axis=1)\n",
    "    return data, y.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def create_simple_sin_dataset(n, p):\n",
    "    '''This dataset was added to provide an example of L1 norm reg failure for presentation.\n",
    "    '''\n",
    "    assert p == 2\n",
    "    x1 = np.random.uniform(-math.pi, math.pi, n).reshape(n ,1)\n",
    "    x2 = np.random.uniform(-math.pi, math.pi, n).reshape(n, 1)\n",
    "    y = np.sin(x1)\n",
    "    data = np.concatenate([x1, x2], axis=1)\n",
    "    print(\"data.shape: {}\".format(data.shape))\n",
    "    return data, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. layers.py\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "__all__ = [\n",
    "    'LinearLayer', 'MLPLayer', 'FeatureSelector',\n",
    "]\n",
    "\n",
    "class FeatureSelector(nn.Module):\n",
    "    def __init__(self, input_dim, sigma, device):\n",
    "        super(FeatureSelector, self).__init__()\n",
    "        self.mu = torch.nn.Parameter(0.01*torch.randn(input_dim, ), requires_grad=True)\n",
    "        self.noise = torch.randn(self.mu.size()) \n",
    "        self.sigma = sigma\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, prev_x):\n",
    "        z = self.mu + self.sigma*self.noise.normal_()*self.training \n",
    "        stochastic_gate = self.hard_sigmoid(z)\n",
    "        new_x = prev_x * stochastic_gate\n",
    "        return new_x\n",
    "    \n",
    "    def hard_sigmoid(self, x):\n",
    "        return torch.clamp(x+0.5, 0.0, 1.0)\n",
    "\n",
    "    def regularizer(self, x):\n",
    "        ''' Gaussian CDF. '''\n",
    "        return 0.5 * (1 + torch.erf(x / math.sqrt(2))) \n",
    "\n",
    "    def _apply(self, fn):\n",
    "        super(FeatureSelector, self)._apply(fn)\n",
    "        self.noise = fn(self.noise)\n",
    "        return self\n",
    "\n",
    "class LinearLayer(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, batch_norm=None, dropout=None, bias=None, activation=None):\n",
    "        if bias is None:\n",
    "            bias = (batch_norm is None)\n",
    "\n",
    "        modules = [nn.Linear(in_features, out_features, bias=bias)]\n",
    "        if batch_norm is not None and batch_norm is not False:\n",
    "            modules.append(get_batcnnorm(batch_norm, out_features, 1))\n",
    "        if dropout is not None and dropout is not False:\n",
    "            modules.append(get_dropout(dropout, 1))\n",
    "        if activation is not None and activation is not False:\n",
    "            modules.append(get_activation(activation))\n",
    "        super().__init__(*modules)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.reset_parameters()\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, batch_norm=None, dropout=None, activation='relu', flatten=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = []\n",
    "        elif type(hidden_dims) is int:\n",
    "            hidden_dims = [hidden_dims]\n",
    "\n",
    "        dims = [input_dim]\n",
    "        dims.extend(hidden_dims)\n",
    "        dims.append(output_dim)\n",
    "        modules = []\n",
    "\n",
    "        nr_hiddens = len(hidden_dims)\n",
    "        for i in range(nr_hiddens):\n",
    "            layer = LinearLayer(dims[i], dims[i+1], batch_norm=batch_norm, dropout=dropout, activation=activation)\n",
    "            modules.append(layer)\n",
    "        layer = nn.Linear(dims[-2], dims[-1], bias=True)\n",
    "        modules.append(layer)\n",
    "        self.mlp = nn.Sequential(*modules)\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.reset_parameters()\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.flatten:\n",
    "            input = input.view(input.size(0), -1)\n",
    "        return self.mlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# from .layers import MLPLayer, FeatureSelector, GatingLayer\n",
    "# from .losses import PartialLogLikelihood\n",
    "\n",
    "__all__ = ['MLPModel', 'MLPRegressionModel', 'MLPClassificationModel', 'LinearRegressionModel', 'LinearClassificationModel']\n",
    "\n",
    "\n",
    "class ModelIOKeysMixin(object):\n",
    "    def _get_input(self, feed_dict):\n",
    "        return feed_dict['input']\n",
    "\n",
    "    def _get_label(self, feed_dict):\n",
    "        return feed_dict['label']\n",
    "\n",
    "    def _get_covariate(self, feed_dict):\n",
    "        '''For cox'''\n",
    "        return feed_dict['X']\n",
    "\n",
    "    def _get_fail_indicator(self, feed_dict):\n",
    "        '''For cox'''\n",
    "        return feed_dict['E'].reshape(-1, 1)\n",
    "\n",
    "    def _get_failure_time(self, feed_dict):\n",
    "        '''For cox'''\n",
    "        return feed_dict['T']\n",
    "\n",
    "    def _compose_output(self, value):\n",
    "        return dict(pred=value)\n",
    "\n",
    "\n",
    "class MLPModel(MLPLayer):\n",
    "    def freeze_weights(self):\n",
    "        for name, p in self.named_parameters():\n",
    "            if name != 'mu':\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def get_gates(self, mode):\n",
    "        if mode == 'raw':\n",
    "            return self.mu.detach().cpu().numpy()\n",
    "        elif mode == 'prob':\n",
    "            return np.minimum(1.0, np.maximum(0.0, self.mu.detach().cpu().numpy() + 0.5)) \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "\n",
    "class STGClassificationModel(MLPModel, ModelIOKeysMixin):\n",
    "    def __init__(self, input_dim, nr_classes, hidden_dims, device, batch_norm=None, dropout=None, activation='relu',\n",
    "                 sigma=1.0, lam=0.1):\n",
    "        super().__init__(input_dim, nr_classes, hidden_dims,\n",
    "                         batch_norm=batch_norm, dropout=dropout, activation=activation)\n",
    "        self.FeatureSelector = FeatureSelector(input_dim, sigma, device)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.reg = self.FeatureSelector.regularizer\n",
    "        self.lam = lam \n",
    "        self.mu = self.FeatureSelector.mu\n",
    "        self.sigma = self.FeatureSelector.sigma\n",
    "        \n",
    "    def forward(self, feed_dict):\n",
    "        x = self.FeatureSelector(self._get_input(feed_dict))\n",
    "        logits = super().forward(x)\n",
    "        if self.training:\n",
    "            loss = self.loss(logits, self._get_label(feed_dict))\n",
    "            reg = torch.mean(self.reg((self.mu + 0.5)/self.sigma)) \n",
    "            total_loss = loss + self.lam * reg \n",
    "            return total_loss, dict(), dict() \n",
    "        else:\n",
    "            return self._compose_output(logits)\n",
    "\n",
    "    def _compose_output(self, logits):\n",
    "        value = self.softmax(logits)\n",
    "        _, pred = value.max(dim=1)\n",
    "        return dict(prob=value, pred=pred, logits=logits)\n",
    "\n",
    "class MLPClassificationModel(MLPModel, ModelIOKeysMixin):\n",
    "    def __init__(self, input_dim, nr_classes, hidden_dims, batch_norm=None, dropout=None, activation='relu'):\n",
    "        super().__init__(input_dim, nr_classes, hidden_dims,\n",
    "                         batch_norm=batch_norm, dropout=dropout, activation=activation)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, feed_dict):\n",
    "        logits = super().forward(self._get_input(feed_dict))\n",
    "        if self.training:\n",
    "            loss = self.loss(logits, self._get_label(feed_dict))\n",
    "            return loss, dict(), dict()\n",
    "        else:\n",
    "            return self._compose_output(logits)\n",
    "\n",
    "    def _compose_output(self, logits):\n",
    "        value = self.softmax(logits)\n",
    "        _, pred = value.max(dim=1)\n",
    "        return dict(prob=value, pred=pred, logits=logits)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meter.py\n",
    "import six\n",
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "\n",
    "def map_exec(func, *iterables):\n",
    "    return list(map(func, *iterables))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    val = 0\n",
    "    avg = 0\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    tot_count = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.tot_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.tot_count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class GroupMeters(object):\n",
    "    def __init__(self):\n",
    "        self._meters = collections.defaultdict(AverageMeter)\n",
    "\n",
    "    def reset(self):\n",
    "        map_exec(AverageMeter.reset, self._meters.values())\n",
    "\n",
    "    def update(self, updates=None, value=None, n=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "            >>> meters.update(key, value)\n",
    "            >>> meters.update({key1: value1, key2: value2})\n",
    "            >>> meters.update(key1=value1, key2=value2)\n",
    "        \"\"\"\n",
    "        if updates is None:\n",
    "            updates = {}\n",
    "        if updates is not None and value is not None:\n",
    "            updates = {updates: value}\n",
    "        updates.update(kwargs)\n",
    "        for k, v in updates.items():\n",
    "            self._meters[k].update(v, n=n)\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self._meters[name]\n",
    "\n",
    "    def items(self):\n",
    "        return self._meters.items()\n",
    "\n",
    "    @property\n",
    "    def sum(self):\n",
    "        return {k: m.sum for k, m in self._meters.items() if m.count > 0}\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return {k: m.avg for k, m in self._meters.items() if m.count > 0}\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return {k: m.val for k, m in self._meters.items() if m.count > 0}\n",
    "\n",
    "    def format(self, caption, values, kv_format, glue):\n",
    "        meters_kv = self._canonize_values(values)\n",
    "        log_str = [caption]\n",
    "        log_str.extend(itertools.starmap(kv_format.format, sorted(meters_kv.items())))\n",
    "        return glue.join(log_str)\n",
    "\n",
    "    def format_simple(self, caption, values='avg', compressed=True):\n",
    "        if compressed:\n",
    "            return self.format(caption, values, '{}={:4f}', ' ')\n",
    "        else:\n",
    "            return self.format(caption, values, '\\t{} = {:4f}', '\\n')\n",
    "\n",
    "    def dump(self, filename, values='avg'):\n",
    "        meters_kv = self._canonize_values(values)\n",
    "        with open(filename, 'a') as f:\n",
    "            #f.write(io.dumps_json(meters_kv, compressed=False))\n",
    "            f.write(json.dumps(meters_kv, cls=JsonObjectEncoder, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "            f.write('\\n')\n",
    "\n",
    "    def _canonize_values(self, values):\n",
    "        if isinstance(values, six.string_types):\n",
    "            assert values in ('avg', 'val', 'sum')\n",
    "            meters_kv = getattr(self, values)\n",
    "        else:\n",
    "            meters_kv = values\n",
    "        return meters_kv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stg.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import logging.config \n",
    "import os.path as osp\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger(\"my-logger\")\n",
    "\n",
    "\n",
    "__all__ = ['STG']\n",
    "\n",
    "\n",
    "def _standard_truncnorm_sample(lower_bound, upper_bound, sample_shape=torch.Size()):\n",
    "    r\"\"\"\n",
    "    Implements accept-reject algorithm for doubly truncated standard normal distribution.\n",
    "    (Section 2.2. Two-sided truncated normal distribution in [1])\n",
    "    [1] Robert, Christian P. \"Simulation of truncated normal variables.\" Statistics and computing 5.2 (1995): 121-125.\n",
    "    Available online: https://arxiv.org/abs/0907.4010\n",
    "    Args:\n",
    "        lower_bound (Tensor): lower bound for standard normal distribution. Best to keep it greater than -4.0 for\n",
    "        stable results\n",
    "        upper_bound (Tensor): upper bound for standard normal distribution. Best to keep it smaller than 4.0 for\n",
    "        stable results\n",
    "    \"\"\"\n",
    "    x = torch.randn(sample_shape)\n",
    "    done = torch.zeros(sample_shape).byte() \n",
    "    while not done.all():\n",
    "        proposed_x = lower_bound + torch.rand(sample_shape) * (upper_bound - lower_bound)\n",
    "        if (upper_bound * lower_bound).lt(0.0):  # of opposite sign\n",
    "            log_prob_accept = -0.5 * proposed_x**2\n",
    "        elif upper_bound < 0.0:  # both negative\n",
    "            log_prob_accept = 0.5 * (upper_bound**2 - proposed_x**2)\n",
    "        else:  # both positive\n",
    "            assert(lower_bound.gt(0.0))\n",
    "            log_prob_accept = 0.5 * (lower_bound**2 - proposed_x**2)\n",
    "        prob_accept = torch.exp(log_prob_accept).clamp_(0.0, 1.0)\n",
    "        accept = torch.bernoulli(prob_accept).byte() & ~done\n",
    "        if accept.any():\n",
    "            accept = accept.bool()\n",
    "            x[accept] = proposed_x[accept]\n",
    "            accept = accept.byte()\n",
    "            done |= accept\n",
    "    return x\n",
    "\n",
    "\n",
    "class STG(object):\n",
    "    def __init__(self, device, input_dim=784, output_dim=10, hidden_dims=[400, 200], activation='relu', sigma=0.5, lam=0.1,\n",
    "                optimizer='Adam', learning_rate=1e-5,  batch_size=100, freeze_onward=None, feature_selection=True, weight_decay=1e-3, \n",
    "                task_type='classification', report_maps=False, random_state=1, extra_args=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.device = self.get_device(device)\n",
    "        self.report_maps = report_maps \n",
    "        self.task_type = task_type\n",
    "        self.extra_args = extra_args\n",
    "        self.freeze_onward = freeze_onward\n",
    "        self._model = self.build_model(input_dim, output_dim, hidden_dims, activation, sigma, lam, \n",
    "                                       task_type, feature_selection)\n",
    "        self._model.apply(self.init_weights)\n",
    "        self._model = self._model.to(device)\n",
    "        self._optimizer = get_optimizer(optimizer, self._model, lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    def get_device(self, device):\n",
    "        if device == \"cpu\":\n",
    "            device = torch.device(\"cpu\")\n",
    "        elif device == None:\n",
    "            args_cuda = torch.cuda.is_available()\n",
    "            device = device = torch.device(\"cuda\" if args_cuda else \"cpu\")\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only 'cpu' or 'cuda' is a valid option.\")\n",
    "        return device\n",
    "        \n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            stddev = torch.tensor(0.1)\n",
    "            shape = m.weight.shape\n",
    "            m.weight = nn.Parameter(_standard_truncnorm_sample(lower_bound=-2*stddev, upper_bound=2*stddev, \n",
    "                                  sample_shape=shape))\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def build_model(self, input_dim, output_dim, hidden_dims, activation, sigma, lam, task_type, feature_selection):\n",
    "        if task_type == 'classification':\n",
    "            self.metric = nn.CrossEntropyLoss()\n",
    "            self.tensor_names = ('input','label')\n",
    "            return STGClassificationModel(input_dim, output_dim, hidden_dims, device=self.device, activation=activation, sigma=sigma, lam=lam)\n",
    "        if task_type == \"reconstruction\":\n",
    "            self.metric = nn.MSELoss()\n",
    "            self.tensor_names = ('input','label')\n",
    "            return STGReconstructionModel(input_dim, output_dim, hidden_dims, device=self.device, activation=activation, sigma=sigma, lam=lam)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def train_step(self, feed_dict, meters=None):\n",
    "        assert self._model.training\n",
    "\n",
    "        loss, logits, monitors = self._model(feed_dict)\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        #probe_infnan(logits, 'logits')\n",
    "        if self.task_type=='cox':\n",
    "            ci = calc_concordance_index(logits.detach().numpy(), \n",
    "                    feed_dict['E'].detach().numpy(), feed_dict['T'].detach().numpy())\n",
    "        if self.extra_args=='l1-softthresh':\n",
    "            self._model.mlp[0][0].weight.data = self._model.prox_op(self._model.mlp[0][0].weight)\n",
    "\n",
    "        loss = as_float(loss)\n",
    "        if meters is not None:\n",
    "            meters.update(loss=loss)\n",
    "            if self.task_type =='cox':\n",
    "                meters.update(CI=ci)\n",
    "            meters.update(monitors)\n",
    "\n",
    "    def get_dataloader(self, X, y, shuffle):\n",
    "        if self.task_type == 'classification' or self.task_type == 'reconstruction':\n",
    "            data_loader = FastTensorDataLoader(torch.from_numpy(X).float().to(self.device), \n",
    "                        torch.from_numpy(y).long().to(self.device), tensor_names=self.tensor_names,\n",
    "                        batch_size=self.batch_size, shuffle=shuffle)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return data_loader \n",
    "\n",
    "    def fit(self, X, y, nr_epochs, valid_X=None, valid_y=None, \n",
    "        verbose=True, meters=None, early_stop=None, print_interval=1, shuffle=False):\n",
    "        data_loader = self.get_dataloader(X, y, shuffle)\n",
    "\n",
    "        if valid_X is not None:\n",
    "            val_data_loader = self.get_dataloader(valid_X, valid_y, shuffle)\n",
    "        else:\n",
    "            val_data_loader = None\n",
    "        self.train(data_loader, nr_epochs, val_data_loader, verbose, meters, early_stop, print_interval)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        data_loader = self.get_dataloader(X, y, shuffle=None)\n",
    "        meters = GroupMeters()\n",
    "        self.validate(data_loader, self.metric, meters, mode='test')\n",
    "        print(meters.format_simple(''))\n",
    "\n",
    "    def predict(self, X, verbose=True):\n",
    "        dataset = SimpleDataset(X)\n",
    "        data_loader = DataLoader(dataset, batch_size=X.shape[0], shuffle=False) \n",
    "        res = []\n",
    "        self._model.eval()\n",
    "        for feed_dict in data_loader:\n",
    "            feed_dict_np = as_numpy(feed_dict)\n",
    "            feed_dict = as_tensor(feed_dict)\n",
    "            with torch.no_grad():\n",
    "                output_dict = self._model(feed_dict)\n",
    "            output_dict_np = as_numpy(output_dict)\n",
    "            res.append(output_dict_np['pred'])\n",
    "        return np.concatenate(res, axis=0)\n",
    "\n",
    "    def train_epoch(self, data_loader, meters=None):\n",
    "        if meters is None:\n",
    "            meters = GroupMeters()\n",
    "\n",
    "        self._model.train()\n",
    "        end = time.time()\n",
    "        for feed_dict in data_loader:\n",
    "            data_time = time.time() - end; end = time.time()\n",
    "            self.train_step(feed_dict, meters=meters)\n",
    "            step_time = time.time() - end; end = time.time()\n",
    "            #if dev:\n",
    "            #meters.update({'time/data': data_time, 'time/step': step_time})\n",
    "        return meters\n",
    "\n",
    "    def train(self, data_loader, nr_epochs, val_data_loader=None, verbose=True, \n",
    "        meters=None, early_stop=None, print_interval=1):\n",
    "        if meters is None:\n",
    "            meters = GroupMeters()\n",
    "\n",
    "        for epoch in range(1, 1 + nr_epochs):\n",
    "            meters.reset()\n",
    "            if epoch == self.freeze_onward:\n",
    "                self._model.freeze_weights()\n",
    "            self.train_epoch(data_loader, meters=meters)\n",
    "            if verbose and epoch % print_interval == 0:\n",
    "                self.validate(val_data_loader, self.metric, meters)\n",
    "                caption = 'Epoch: {}:'.format(epoch)\n",
    "                print(meters.format_simple(caption))\n",
    "            if early_stop is not None:\n",
    "                flag = early_stop(self._model)\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "    def validate_step(self, feed_dict, metric, meters=None, mode='valid'):\n",
    "        with torch.no_grad():\n",
    "            pred = self._model(feed_dict)\n",
    "        if self.task_type == 'classification':\n",
    "            result = metric(pred['logits'], self._model._get_label(feed_dict))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if meters is not None:\n",
    "            meters.update({mode+'_loss':result})\n",
    "            if self.task_type=='cox':\n",
    "                meters.update({mode+'_CI':val_CI})\n",
    "\n",
    "    def validate(self, data_loader, metric, meters=None, mode='valid'):\n",
    "        if meters is None:\n",
    "            meters = GroupMeters()\n",
    "\n",
    "        self._model.eval()\n",
    "        end = time.time()\n",
    "        for fd in data_loader:\n",
    "            data_time = time.time() - end; end = time.time()\n",
    "            self.validate_step(fd, metric, meters=meters, mode=mode)\n",
    "            step_time = time.time() - end; end = time.time()\n",
    "\n",
    "        return meters.avg\n",
    "\n",
    "    def get_gates(self, mode):\n",
    "        return self._model.get_gates(mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_cuda = torch.cuda.is_available()\n",
    "device = \"cuda\" if args_cuda else \"cpu\"\n",
    "feature_selection = True\n",
    "model = STG(task_type='classification',input_dim=X_train.shape[1], output_dim=2, hidden_dims=[200, 60], activation='relu',\n",
    "    optimizer='Adam', learning_rate=0.01, batch_size=X_train.shape[0], feature_selection=feature_selection, sigma=0.5, lam=0.5, random_state=1, device=device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:215: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "/Users/mamun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:217: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "/Users/mamun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:219: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "/Users/mamun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.342700 valid_loss=0.002340\n",
      "Epoch: 400: loss=0.327726 valid_loss=0.032171\n",
      "Epoch: 600: loss=0.320385 valid_loss=0.248116\n",
      "Epoch: 800: loss=0.309509 valid_loss=0.044702\n",
      "Epoch: 1000: loss=0.310075 valid_loss=0.014132\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train.to_numpy(), y_train.to_numpy(), nr_epochs=1000, valid_X=X_valid.to_numpy(), valid_y=y_valid.to_numpy(), print_interval=200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022\n",
      "(43, 1.0)\n",
      "(317, 1.0)\n",
      "(385, 1.0)\n",
      "(509, 1.0)\n",
      "(512, 1.0)\n",
      "(620, 1.0)\n",
      "(628, 1.0)\n",
      "(663, 1.0)\n",
      "(766, 1.0)\n",
      "(262, 0.9686836)\n",
      "(490, 0.95932406)\n",
      "(866, 0.87786764)\n",
      "(73, 0.8730328)\n",
      "(716, 0.86529917)\n",
      "(436, 0.84358597)\n",
      "(664, 0.810651)\n",
      "(633, 0.7933525)\n",
      "(922, 0.7923891)\n",
      "(960, 0.7917078)\n",
      "(99, 0.7598001)\n",
      "(890, 0.7498144)\n",
      "(410, 0.7340355)\n",
      "(77, 0.7004177)\n",
      "(26, 0.69604516)\n",
      "(1001, 0.65572035)\n",
      "(399, 0.6446345)\n",
      "(964, 0.58424264)\n",
      "(672, 0.5591872)\n",
      "(38, 0.54006386)\n",
      "(774, 0.53479934)\n",
      "(962, 0.5325687)\n",
      "(465, 0.5055366)\n",
      "(54, 0.50200444)\n",
      "(417, 0.49159747)\n",
      "(454, 0.49098492)\n",
      "(703, 0.48926735)\n",
      "(397, 0.4850421)\n",
      "(982, 0.47453162)\n",
      "(941, 0.47258866)\n",
      "(411, 0.46717504)\n",
      "(673, 0.45691586)\n",
      "(837, 0.45154312)\n",
      "(169, 0.44324625)\n",
      "(1013, 0.43830755)\n",
      "(215, 0.4356114)\n",
      "(190, 0.4222411)\n",
      "(370, 0.41159254)\n",
      "(714, 0.39193904)\n",
      "(113, 0.38729998)\n",
      "(200, 0.38551196)\n",
      "(697, 0.383628)\n",
      "(625, 0.3786059)\n",
      "(81, 0.3716334)\n",
      "(969, 0.36709508)\n",
      "(278, 0.36585626)\n",
      "(311, 0.36296427)\n",
      "(282, 0.36209983)\n",
      "(63, 0.35531992)\n",
      "(510, 0.35415483)\n",
      "(452, 0.3511464)\n",
      "(297, 0.3440026)\n",
      "(393, 0.3363974)\n",
      "(724, 0.3351636)\n",
      "(747, 0.33501172)\n",
      "(51, 0.3331741)\n",
      "(60, 0.32821324)\n",
      "(608, 0.32349163)\n",
      "(438, 0.32267648)\n",
      "(940, 0.31950504)\n",
      "(784, 0.31708533)\n",
      "(171, 0.3151793)\n",
      "(511, 0.3097835)\n",
      "(373, 0.3075173)\n",
      "(758, 0.30115741)\n",
      "(492, 0.299657)\n",
      "(369, 0.2887106)\n",
      "(634, 0.2848209)\n",
      "(799, 0.284132)\n",
      "(449, 0.28160557)\n",
      "(229, 0.2798071)\n",
      "(263, 0.27915627)\n",
      "(85, 0.2790734)\n",
      "(322, 0.2756812)\n",
      "(403, 0.2732601)\n",
      "(247, 0.27273488)\n",
      "(258, 0.2710657)\n",
      "(291, 0.26196778)\n",
      "(147, 0.2584986)\n",
      "(187, 0.25779474)\n",
      "(477, 0.25073513)\n",
      "(248, 0.25039113)\n",
      "(168, 0.24981779)\n",
      "(437, 0.24567929)\n",
      "(779, 0.24517304)\n",
      "(23, 0.24142495)\n",
      "(67, 0.24076056)\n",
      "(394, 0.23683396)\n",
      "(0, 0.2311886)\n",
      "(760, 0.22990286)\n",
      "(111, 0.22918576)\n",
      "(16, 0.22706884)\n",
      "(725, 0.2257193)\n",
      "(555, 0.22477442)\n",
      "(160, 0.22064853)\n",
      "(446, 0.22038263)\n",
      "(732, 0.21789584)\n",
      "(657, 0.21582508)\n",
      "(46, 0.21304947)\n",
      "(901, 0.21140695)\n",
      "(889, 0.21045446)\n",
      "(196, 0.21008188)\n",
      "(671, 0.20972216)\n",
      "(180, 0.20896631)\n",
      "(993, 0.20660266)\n",
      "(197, 0.20470777)\n",
      "(131, 0.20405337)\n",
      "(637, 0.20349896)\n",
      "(738, 0.20341909)\n",
      "(513, 0.203006)\n",
      "(859, 0.20236206)\n",
      "(482, 0.20215452)\n",
      "(13, 0.2018398)\n",
      "(624, 0.20178422)\n",
      "(89, 0.20138097)\n",
      "(825, 0.20112768)\n",
      "(945, 0.20018902)\n",
      "(379, 0.19914529)\n",
      "(118, 0.19791439)\n",
      "(806, 0.19785133)\n",
      "(1, 0.19638687)\n",
      "(39, 0.19452345)\n",
      "(228, 0.19397461)\n",
      "(122, 0.19327241)\n",
      "(888, 0.19323581)\n",
      "(503, 0.19292274)\n",
      "(639, 0.19271013)\n",
      "(609, 0.19256622)\n",
      "(301, 0.19242927)\n",
      "(245, 0.19236293)\n",
      "(967, 0.19215137)\n",
      "(815, 0.19180074)\n",
      "(615, 0.19068974)\n",
      "(876, 0.18966123)\n",
      "(464, 0.18938005)\n",
      "(601, 0.1851894)\n",
      "(857, 0.1836823)\n",
      "(25, 0.182251)\n",
      "(217, 0.18118396)\n",
      "(915, 0.18082163)\n",
      "(892, 0.1807907)\n",
      "(33, 0.1796591)\n",
      "(996, 0.17686)\n",
      "(143, 0.17353171)\n",
      "(199, 0.17318073)\n",
      "(415, 0.172461)\n",
      "(553, 0.17228588)\n",
      "(225, 0.17034712)\n",
      "(457, 0.17018661)\n",
      "(281, 0.16591212)\n",
      "(325, 0.16354865)\n",
      "(868, 0.16331258)\n",
      "(157, 0.16277748)\n",
      "(619, 0.16224274)\n",
      "(357, 0.16222885)\n",
      "(184, 0.161719)\n",
      "(420, 0.16112664)\n",
      "(267, 0.16082472)\n",
      "(427, 0.1600467)\n",
      "(28, 0.15979594)\n",
      "(613, 0.15971833)\n",
      "(386, 0.15857074)\n",
      "(983, 0.157146)\n",
      "(232, 0.15666988)\n",
      "(155, 0.15655628)\n",
      "(132, 0.15652588)\n",
      "(742, 0.15629888)\n",
      "(231, 0.15485221)\n",
      "(109, 0.15371203)\n",
      "(809, 0.15346536)\n",
      "(842, 0.15328506)\n",
      "(305, 0.15290195)\n",
      "(272, 0.15285099)\n",
      "(253, 0.15280095)\n",
      "(243, 0.15217164)\n",
      "(87, 0.15206677)\n",
      "(471, 0.1500679)\n",
      "(421, 0.14764172)\n",
      "(959, 0.14741647)\n",
      "(296, 0.14739901)\n",
      "(720, 0.14595866)\n",
      "(201, 0.1453003)\n",
      "(698, 0.14522147)\n",
      "(395, 0.14514285)\n",
      "(526, 0.14513862)\n",
      "(161, 0.14505184)\n",
      "(340, 0.1428231)\n",
      "(843, 0.14256668)\n",
      "(795, 0.1419293)\n",
      "(260, 0.14188409)\n",
      "(707, 0.14149112)\n",
      "(230, 0.1411646)\n",
      "(242, 0.14100817)\n",
      "(640, 0.13990858)\n",
      "(136, 0.13973194)\n",
      "(560, 0.13938493)\n",
      "(456, 0.13869095)\n",
      "(778, 0.13833845)\n",
      "(435, 0.13792306)\n",
      "(312, 0.13713169)\n",
      "(388, 0.13694733)\n",
      "(577, 0.13667122)\n",
      "(407, 0.13650289)\n",
      "(656, 0.13637343)\n",
      "(708, 0.13635811)\n",
      "(396, 0.13621774)\n",
      "(64, 0.13535911)\n",
      "(466, 0.13495499)\n",
      "(264, 0.1347425)\n",
      "(6, 0.1346432)\n",
      "(914, 0.1345396)\n",
      "(562, 0.13452306)\n",
      "(350, 0.13449112)\n",
      "(227, 0.13437536)\n",
      "(547, 0.13425681)\n",
      "(97, 0.13328734)\n",
      "(486, 0.13319057)\n",
      "(533, 0.13299018)\n",
      "(919, 0.13291076)\n",
      "(78, 0.13260427)\n",
      "(50, 0.13243136)\n",
      "(55, 0.1318593)\n",
      "(125, 0.13182953)\n",
      "(804, 0.13164559)\n",
      "(767, 0.13164443)\n",
      "(934, 0.13164291)\n",
      "(468, 0.13158059)\n",
      "(119, 0.1314694)\n",
      "(485, 0.13092637)\n",
      "(59, 0.13069704)\n",
      "(69, 0.13047242)\n",
      "(958, 0.13046256)\n",
      "(812, 0.13024867)\n",
      "(905, 0.13015512)\n",
      "(559, 0.13000381)\n",
      "(632, 0.13000134)\n",
      "(462, 0.1299867)\n",
      "(151, 0.12949693)\n",
      "(885, 0.12928557)\n",
      "(35, 0.12925515)\n",
      "(968, 0.12922448)\n",
      "(280, 0.1290982)\n",
      "(753, 0.12908304)\n",
      "(629, 0.12891743)\n",
      "(104, 0.12873957)\n",
      "(76, 0.12873676)\n",
      "(86, 0.12867302)\n",
      "(791, 0.1284506)\n",
      "(502, 0.12834519)\n",
      "(308, 0.12817019)\n",
      "(170, 0.1277959)\n",
      "(718, 0.12771487)\n",
      "(1000, 0.12764382)\n",
      "(159, 0.12757698)\n",
      "(304, 0.12739423)\n",
      "(206, 0.12738052)\n",
      "(700, 0.1273587)\n",
      "(348, 0.12729469)\n",
      "(164, 0.12726456)\n",
      "(61, 0.12724611)\n",
      "(875, 0.12706816)\n",
      "(528, 0.12680906)\n",
      "(178, 0.12666196)\n",
      "(105, 0.12627956)\n",
      "(1019, 0.12623149)\n",
      "(858, 0.12603351)\n",
      "(24, 0.12594375)\n",
      "(688, 0.12594193)\n",
      "(515, 0.12579262)\n",
      "(383, 0.12565348)\n",
      "(572, 0.12539941)\n",
      "(895, 0.12530077)\n",
      "(114, 0.1251747)\n",
      "(907, 0.12517336)\n",
      "(668, 0.12499541)\n",
      "(106, 0.124913484)\n",
      "(852, 0.1247952)\n",
      "(265, 0.12474108)\n",
      "(552, 0.12463924)\n",
      "(648, 0.124545366)\n",
      "(121, 0.124394596)\n",
      "(519, 0.12432665)\n",
      "(995, 0.124310166)\n",
      "(57, 0.12422848)\n",
      "(873, 0.12416577)\n",
      "(950, 0.12413719)\n",
      "(911, 0.12409675)\n",
      "(507, 0.124044746)\n",
      "(1007, 0.12395343)\n",
      "(660, 0.12385577)\n",
      "(252, 0.12379131)\n",
      "(93, 0.123741716)\n",
      "(176, 0.12367439)\n",
      "(928, 0.12364799)\n",
      "(2, 0.12358335)\n",
      "(255, 0.12345022)\n",
      "(970, 0.12340182)\n",
      "(971, 0.12334621)\n",
      "(288, 0.12334296)\n",
      "(494, 0.12323567)\n",
      "(18, 0.12311369)\n",
      "(32, 0.12310356)\n",
      "(362, 0.123042315)\n",
      "(173, 0.12301472)\n",
      "(213, 0.12297553)\n",
      "(284, 0.12294799)\n",
      "(358, 0.12288481)\n",
      "(721, 0.12284136)\n",
      "(596, 0.122777015)\n",
      "(341, 0.12270343)\n",
      "(514, 0.1226905)\n",
      "(406, 0.12264508)\n",
      "(581, 0.122630745)\n",
      "(832, 0.12262902)\n",
      "(994, 0.12262672)\n",
      "(283, 0.12261975)\n",
      "(706, 0.12258762)\n",
      "(614, 0.122561306)\n",
      "(777, 0.12243667)\n",
      "(285, 0.122359216)\n",
      "(363, 0.122335225)\n",
      "(735, 0.122289926)\n",
      "(321, 0.12223461)\n",
      "(37, 0.12222469)\n",
      "(847, 0.12221074)\n",
      "(249, 0.12219983)\n",
      "(266, 0.12215334)\n",
      "(1009, 0.12214339)\n",
      "(826, 0.122131914)\n",
      "(218, 0.122112334)\n",
      "(495, 0.12208834)\n",
      "(107, 0.12206662)\n",
      "(10, 0.12203634)\n",
      "(34, 0.121985614)\n",
      "(333, 0.1219832)\n",
      "(683, 0.12197158)\n",
      "(430, 0.121965915)\n",
      "(728, 0.12196487)\n",
      "(710, 0.12193301)\n",
      "(1018, 0.12188089)\n",
      "(853, 0.12186679)\n",
      "(185, 0.12184057)\n",
      "(460, 0.12181029)\n",
      "(133, 0.12177718)\n",
      "(808, 0.12176418)\n",
      "(979, 0.12174624)\n",
      "(641, 0.12174374)\n",
      "(649, 0.12174162)\n",
      "(484, 0.12172294)\n",
      "(289, 0.121719)\n",
      "(165, 0.12167162)\n",
      "(162, 0.12164724)\n",
      "(751, 0.12161112)\n",
      "(818, 0.12160164)\n",
      "(680, 0.12159425)\n",
      "(140, 0.121576965)\n",
      "(600, 0.121548146)\n",
      "(177, 0.121545434)\n",
      "(831, 0.12153059)\n",
      "(315, 0.12151289)\n",
      "(1005, 0.121474236)\n",
      "(429, 0.12146959)\n",
      "(496, 0.12145609)\n",
      "(268, 0.12144497)\n",
      "(647, 0.12141517)\n",
      "(652, 0.12140742)\n",
      "(845, 0.121406734)\n",
      "(797, 0.12140432)\n",
      "(433, 0.12134966)\n",
      "(146, 0.1213457)\n",
      "(239, 0.121341795)\n",
      "(479, 0.121317655)\n",
      "(597, 0.12130964)\n",
      "(622, 0.12130529)\n",
      "(355, 0.121290535)\n",
      "(148, 0.121280074)\n",
      "(543, 0.12127897)\n",
      "(331, 0.12127152)\n",
      "(95, 0.12126532)\n",
      "(881, 0.12126264)\n",
      "(332, 0.12125003)\n",
      "(923, 0.121234834)\n",
      "(855, 0.121224195)\n",
      "(158, 0.121224165)\n",
      "(251, 0.121216774)\n",
      "(374, 0.121210545)\n",
      "(618, 0.12121025)\n",
      "(856, 0.12119934)\n",
      "(909, 0.1211845)\n",
      "(988, 0.12115997)\n",
      "(935, 0.12115747)\n",
      "(975, 0.12115657)\n",
      "(749, 0.121150285)\n",
      "(453, 0.12114051)\n",
      "(611, 0.12113762)\n",
      "(222, 0.1211358)\n",
      "(631, 0.12112683)\n",
      "(328, 0.12112579)\n",
      "(293, 0.12111455)\n",
      "(66, 0.121106565)\n",
      "(796, 0.12110096)\n",
      "(896, 0.121091306)\n",
      "(318, 0.12108919)\n",
      "(786, 0.12108332)\n",
      "(954, 0.12107968)\n",
      "(188, 0.12107882)\n",
      "(524, 0.12106264)\n",
      "(811, 0.12106225)\n",
      "(71, 0.121055394)\n",
      "(594, 0.121049374)\n",
      "(908, 0.12104455)\n",
      "(650, 0.121041834)\n",
      "(900, 0.121040344)\n",
      "(727, 0.12103331)\n",
      "(891, 0.12103161)\n",
      "(884, 0.12103057)\n",
      "(947, 0.121027976)\n",
      "(534, 0.12102321)\n",
      "(574, 0.12102279)\n",
      "(141, 0.121017426)\n",
      "(687, 0.12101528)\n",
      "(734, 0.1210148)\n",
      "(882, 0.12101334)\n",
      "(142, 0.12100452)\n",
      "(575, 0.121001095)\n",
      "(472, 0.1209988)\n",
      "(175, 0.12099263)\n",
      "(8, 0.12098849)\n",
      "(719, 0.12098727)\n",
      "(550, 0.120980114)\n",
      "(661, 0.120975465)\n",
      "(569, 0.120964944)\n",
      "(461, 0.120963186)\n",
      "(737, 0.12095955)\n",
      "(785, 0.12095514)\n",
      "(343, 0.120954454)\n",
      "(781, 0.12095103)\n",
      "(655, 0.12094578)\n",
      "(584, 0.12094548)\n",
      "(604, 0.12094414)\n",
      "(726, 0.120940864)\n",
      "(323, 0.12093988)\n",
      "(541, 0.12093642)\n",
      "(205, 0.12093601)\n",
      "(316, 0.120934546)\n",
      "(250, 0.120933294)\n",
      "(689, 0.12093094)\n",
      "(621, 0.1209293)\n",
      "(739, 0.120927095)\n",
      "(792, 0.12092644)\n",
      "(1021, 0.12092584)\n",
      "(368, 0.120924264)\n",
      "(579, 0.120924145)\n",
      "(346, 0.12092319)\n",
      "(829, 0.12092075)\n",
      "(603, 0.12091714)\n",
      "(287, 0.120916486)\n",
      "(274, 0.120915234)\n",
      "(589, 0.12091479)\n",
      "(828, 0.12091437)\n",
      "(929, 0.120912254)\n",
      "(565, 0.12091163)\n",
      "(102, 0.12091091)\n",
      "(570, 0.120909095)\n",
      "(605, 0.120907605)\n",
      "(270, 0.12090725)\n",
      "(203, 0.12090507)\n",
      "(898, 0.12090385)\n",
      "(21, 0.120903224)\n",
      "(669, 0.12090224)\n",
      "(874, 0.12090173)\n",
      "(981, 0.12090054)\n",
      "(715, 0.12090042)\n",
      "(838, 0.1209003)\n",
      "(924, 0.12090027)\n",
      "(463, 0.120899886)\n",
      "(256, 0.12089935)\n",
      "(469, 0.120898634)\n",
      "(867, 0.12089759)\n",
      "(691, 0.12089643)\n",
      "(144, 0.12089622)\n",
      "(204, 0.12089527)\n",
      "(1003, 0.12089518)\n",
      "(49, 0.12089506)\n",
      "(713, 0.12089464)\n",
      "(149, 0.12089455)\n",
      "(761, 0.12089455)\n",
      "(334, 0.120894104)\n",
      "(864, 0.120894074)\n",
      "(497, 0.12089333)\n",
      "(925, 0.12089327)\n",
      "(705, 0.120892465)\n",
      "(684, 0.120892406)\n",
      "(365, 0.12089202)\n",
      "(474, 0.12089175)\n",
      "(782, 0.12089169)\n",
      "(413, 0.1208916)\n",
      "(678, 0.12089145)\n",
      "(391, 0.120891094)\n",
      "(670, 0.120891005)\n",
      "(478, 0.120890826)\n",
      "(19, 0.12089068)\n",
      "(62, 0.12089068)\n",
      "(743, 0.12089059)\n",
      "(998, 0.12089059)\n",
      "(47, 0.1208905)\n",
      "(677, 0.12089035)\n",
      "(556, 0.12089032)\n",
      "(459, 0.12089026)\n",
      "(816, 0.12089026)\n",
      "(83, 0.12089023)\n",
      "(139, 0.12089023)\n",
      "(583, 0.12089023)\n",
      "(470, 0.1208902)\n",
      "(20, 0.12089014)\n",
      "(366, 0.12089008)\n",
      "(120, 0.12089002)\n",
      "(15, 0.12088999)\n",
      "(17, 0.12088999)\n",
      "(30, 0.12088999)\n",
      "(152, 0.12088999)\n",
      "(377, 0.12088999)\n",
      "(381, 0.12088999)\n",
      "(382, 0.12088999)\n",
      "(439, 0.12088999)\n",
      "(576, 0.12088999)\n",
      "(696, 0.12088999)\n",
      "(912, 0.12088999)\n",
      "(948, 0.12088999)\n",
      "(973, 0.12088999)\n",
      "(1008, 0.12088999)\n",
      "(14, 0.12088996)\n",
      "(29, 0.12088996)\n",
      "(90, 0.12088996)\n",
      "(192, 0.12088996)\n",
      "(544, 0.12088996)\n",
      "(675, 0.12088996)\n",
      "(679, 0.12088996)\n",
      "(913, 0.12088996)\n",
      "(516, 0.12088969)\n",
      "(740, 0.120889544)\n",
      "(354, 0.120889515)\n",
      "(803, 0.120889336)\n",
      "(554, 0.120889276)\n",
      "(505, 0.1208888)\n",
      "(378, 0.12088874)\n",
      "(290, 0.12088859)\n",
      "(933, 0.12088841)\n",
      "(92, 0.12088835)\n",
      "(699, 0.12088835)\n",
      "(246, 0.12088817)\n",
      "(821, 0.12088752)\n",
      "(757, 0.12088743)\n",
      "(887, 0.1208874)\n",
      "(850, 0.12088734)\n",
      "(440, 0.12088725)\n",
      "(481, 0.12088707)\n",
      "(790, 0.12088618)\n",
      "(493, 0.12088588)\n",
      "(45, 0.12088543)\n",
      "(345, 0.12088531)\n",
      "(879, 0.120885015)\n",
      "(521, 0.12088436)\n",
      "(607, 0.12088418)\n",
      "(772, 0.12088412)\n",
      "(723, 0.12088409)\n",
      "(310, 0.12088394)\n",
      "(1011, 0.12088382)\n",
      "(763, 0.120883465)\n",
      "(756, 0.1208829)\n",
      "(917, 0.12088233)\n",
      "(500, 0.120881945)\n",
      "(771, 0.120881945)\n",
      "(894, 0.12088162)\n",
      "(854, 0.12088138)\n",
      "(538, 0.12088132)\n",
      "(347, 0.12088087)\n",
      "(776, 0.12088072)\n",
      "(342, 0.120880365)\n",
      "(306, 0.12088004)\n",
      "(980, 0.12087968)\n",
      "(530, 0.12087953)\n",
      "(241, 0.1208792)\n",
      "(226, 0.120878994)\n",
      "(862, 0.120878845)\n",
      "(773, 0.12087864)\n",
      "(522, 0.12087846)\n",
      "(794, 0.12087774)\n",
      "(802, 0.120877475)\n",
      "(295, 0.12087667)\n",
      "(918, 0.120875776)\n",
      "(573, 0.12087515)\n",
      "(198, 0.1208747)\n",
      "(953, 0.1208739)\n",
      "(978, 0.12087357)\n",
      "(955, 0.12087166)\n",
      "(946, 0.120870024)\n",
      "(504, 0.12086862)\n",
      "(1002, 0.12086859)\n",
      "(154, 0.120868206)\n",
      "(830, 0.12086812)\n",
      "(844, 0.12086755)\n",
      "(167, 0.12086734)\n",
      "(991, 0.12086719)\n",
      "(949, 0.12086648)\n",
      "(659, 0.1208663)\n",
      "(638, 0.12086582)\n",
      "(145, 0.12086427)\n",
      "(224, 0.12086183)\n",
      "(578, 0.12086117)\n",
      "(445, 0.12086061)\n",
      "(558, 0.12086007)\n",
      "(5, 0.12085536)\n",
      "(899, 0.120853364)\n",
      "(186, 0.12085298)\n",
      "(817, 0.12085292)\n",
      "(352, 0.12085289)\n",
      "(330, 0.12085229)\n",
      "(491, 0.12085202)\n",
      "(273, 0.12085095)\n",
      "(371, 0.12085047)\n",
      "(98, 0.120850116)\n",
      "(549, 0.1208491)\n",
      "(646, 0.120847374)\n",
      "(408, 0.12084472)\n",
      "(476, 0.120844126)\n",
      "(801, 0.120842755)\n",
      "(11, 0.12084022)\n",
      "(335, 0.120839804)\n",
      "(931, 0.12083906)\n",
      "(956, 0.120838195)\n",
      "(299, 0.12083572)\n",
      "(630, 0.12083563)\n",
      "(400, 0.12083462)\n",
      "(789, 0.1208345)\n",
      "(770, 0.12083435)\n",
      "(489, 0.12083277)\n",
      "(238, 0.12082982)\n",
      "(319, 0.12082943)\n",
      "(685, 0.120827466)\n",
      "(27, 0.12082577)\n",
      "(681, 0.12082535)\n",
      "(501, 0.120818526)\n",
      "(987, 0.12081757)\n",
      "(653, 0.1208159)\n",
      "(903, 0.12081528)\n",
      "(52, 0.12081444)\n",
      "(277, 0.12081435)\n",
      "(566, 0.12081057)\n",
      "(257, 0.12081051)\n",
      "(944, 0.12080696)\n",
      "(127, 0.120806724)\n",
      "(563, 0.120806456)\n",
      "(775, 0.12080607)\n",
      "(665, 0.12080523)\n",
      "(800, 0.120805025)\n",
      "(1017, 0.12080291)\n",
      "(986, 0.12080228)\n",
      "(353, 0.12080127)\n",
      "(275, 0.120794326)\n",
      "(303, 0.12079391)\n",
      "(202, 0.120792925)\n",
      "(207, 0.12079242)\n",
      "(48, 0.12078479)\n",
      "(244, 0.12078327)\n",
      "(434, 0.12078217)\n",
      "(704, 0.120780766)\n",
      "(234, 0.12077698)\n",
      "(824, 0.12077317)\n",
      "(645, 0.1207701)\n",
      "(886, 0.12076694)\n",
      "(733, 0.120759934)\n",
      "(44, 0.12075871)\n",
      "(635, 0.12075442)\n",
      "(610, 0.120753735)\n",
      "(183, 0.1207515)\n",
      "(210, 0.120749)\n",
      "(568, 0.120741785)\n",
      "(216, 0.12073955)\n",
      "(755, 0.1207256)\n",
      "(788, 0.12072456)\n",
      "(261, 0.12072033)\n",
      "(103, 0.12071964)\n",
      "(209, 0.12071875)\n",
      "(904, 0.12070966)\n",
      "(585, 0.12070501)\n",
      "(240, 0.12069258)\n",
      "(220, 0.12068987)\n",
      "(483, 0.12068939)\n",
      "(320, 0.12067908)\n",
      "(432, 0.120663166)\n",
      "(31, 0.12066251)\n",
      "(172, 0.12065646)\n",
      "(101, 0.120652586)\n",
      "(327, 0.12064901)\n",
      "(750, 0.12064651)\n",
      "(651, 0.120645046)\n",
      "(642, 0.120642245)\n",
      "(174, 0.120639145)\n",
      "(768, 0.12063891)\n",
      "(108, 0.120634556)\n",
      "(690, 0.12062484)\n",
      "(529, 0.12061989)\n",
      "(404, 0.120614946)\n",
      "(424, 0.120606214)\n",
      "(780, 0.12060547)\n",
      "(759, 0.12059572)\n",
      "(179, 0.12058014)\n",
      "(442, 0.12057477)\n",
      "(329, 0.12057117)\n",
      "(926, 0.120563835)\n",
      "(748, 0.120563775)\n",
      "(840, 0.12055859)\n",
      "(498, 0.1205568)\n",
      "(349, 0.12055379)\n",
      "(191, 0.12054813)\n",
      "(150, 0.12054351)\n",
      "(42, 0.12054226)\n",
      "(823, 0.120539635)\n",
      "(423, 0.12053791)\n",
      "(1010, 0.12052193)\n",
      "(939, 0.12051475)\n",
      "(877, 0.12050265)\n",
      "(686, 0.1205022)\n",
      "(930, 0.12048626)\n",
      "(730, 0.12048456)\n",
      "(360, 0.120480835)\n",
      "(80, 0.12047747)\n",
      "(626, 0.120464385)\n",
      "(871, 0.12045705)\n",
      "(920, 0.12042466)\n",
      "(722, 0.1204105)\n",
      "(212, 0.120404035)\n",
      "(564, 0.12039769)\n",
      "(193, 0.12039325)\n",
      "(861, 0.12039238)\n",
      "(532, 0.12037626)\n",
      "(72, 0.12037444)\n",
      "(128, 0.120372474)\n",
      "(643, 0.12036878)\n",
      "(612, 0.12035924)\n",
      "(966, 0.12035644)\n",
      "(163, 0.12034947)\n",
      "(189, 0.12034467)\n",
      "(326, 0.12030169)\n",
      "(841, 0.12029573)\n",
      "(153, 0.120295316)\n",
      "(580, 0.120294005)\n",
      "(588, 0.1202811)\n",
      "(1012, 0.12027186)\n",
      "(586, 0.12026855)\n",
      "(745, 0.12026197)\n",
      "(1015, 0.120171815)\n",
      "(91, 0.12016484)\n",
      "(361, 0.120138824)\n",
      "(752, 0.12012926)\n",
      "(269, 0.1201269)\n",
      "(259, 0.12010273)\n",
      "(520, 0.12010145)\n",
      "(302, 0.12009844)\n",
      "(344, 0.120084494)\n",
      "(571, 0.120079845)\n",
      "(487, 0.120060086)\n",
      "(419, 0.120052665)\n",
      "(598, 0.12003836)\n",
      "(237, 0.12002289)\n",
      "(711, 0.12000874)\n",
      "(951, 0.12000647)\n",
      "(74, 0.11998665)\n",
      "(75, 0.11998254)\n",
      "(977, 0.11996427)\n",
      "(351, 0.11995691)\n",
      "(906, 0.11995399)\n",
      "(359, 0.11994222)\n",
      "(441, 0.11993611)\n",
      "(292, 0.11991173)\n",
      "(992, 0.11987254)\n",
      "(134, 0.119855404)\n",
      "(115, 0.11982575)\n",
      "(409, 0.119793385)\n",
      "(518, 0.11976108)\n",
      "(254, 0.11973864)\n",
      "(428, 0.11973572)\n",
      "(235, 0.11959818)\n",
      "(181, 0.119569)\n",
      "(551, 0.11952683)\n",
      "(827, 0.11944732)\n",
      "(412, 0.11944112)\n",
      "(916, 0.11944088)\n",
      "(223, 0.11941165)\n",
      "(833, 0.11939779)\n",
      "(712, 0.119326144)\n",
      "(387, 0.11931282)\n",
      "(375, 0.11929339)\n",
      "(990, 0.11923608)\n",
      "(717, 0.11923534)\n",
      "(810, 0.1192112)\n",
      "(130, 0.119185954)\n",
      "(79, 0.11918494)\n",
      "(936, 0.119184434)\n",
      "(364, 0.11911002)\n",
      "(40, 0.11909014)\n",
      "(12, 0.119062066)\n",
      "(762, 0.119037926)\n",
      "(324, 0.11903763)\n",
      "(117, 0.11901331)\n",
      "(525, 0.1189)\n",
      "(965, 0.11885309)\n",
      "(537, 0.11883277)\n",
      "(595, 0.118820965)\n",
      "(535, 0.11880121)\n",
      "(70, 0.11872256)\n",
      "(425, 0.11870071)\n",
      "(467, 0.11869958)\n",
      "(910, 0.11868787)\n",
      "(744, 0.118614286)\n",
      "(444, 0.118599296)\n",
      "(787, 0.11853221)\n",
      "(94, 0.11851248)\n",
      "(398, 0.118501455)\n",
      "(112, 0.11842698)\n",
      "(623, 0.11841336)\n",
      "(834, 0.1183601)\n",
      "(473, 0.11835992)\n",
      "(384, 0.11830419)\n",
      "(401, 0.11829218)\n",
      "(455, 0.118281096)\n",
      "(746, 0.11826423)\n",
      "(872, 0.11825979)\n",
      "(548, 0.11823481)\n",
      "(9, 0.118171394)\n",
      "(208, 0.118122995)\n",
      "(116, 0.11792436)\n",
      "(286, 0.1179232)\n",
      "(921, 0.117918015)\n",
      "(754, 0.11788666)\n",
      "(736, 0.11783779)\n",
      "(338, 0.11765975)\n",
      "(135, 0.11757231)\n",
      "(1020, 0.11756903)\n",
      "(616, 0.11753839)\n",
      "(805, 0.117516756)\n",
      "(695, 0.117452145)\n",
      "(943, 0.11743319)\n",
      "(211, 0.117428124)\n",
      "(166, 0.11739421)\n",
      "(602, 0.1171689)\n",
      "(883, 0.11716542)\n",
      "(480, 0.11714542)\n",
      "(957, 0.11709377)\n",
      "(731, 0.116911024)\n",
      "(590, 0.11690807)\n",
      "(985, 0.11683428)\n",
      "(156, 0.116809666)\n",
      "(938, 0.11672881)\n",
      "(88, 0.1166445)\n",
      "(123, 0.11657274)\n",
      "(1004, 0.11652425)\n",
      "(851, 0.11642161)\n",
      "(682, 0.11639354)\n",
      "(557, 0.11635718)\n",
      "(58, 0.11631495)\n",
      "(606, 0.11628869)\n",
      "(976, 0.116200894)\n",
      "(22, 0.11613786)\n",
      "(458, 0.115980834)\n",
      "(416, 0.11594167)\n",
      "(617, 0.115885854)\n",
      "(819, 0.11586487)\n",
      "(542, 0.115848035)\n",
      "(561, 0.11572793)\n",
      "(592, 0.11568275)\n",
      "(517, 0.11563277)\n",
      "(963, 0.115502715)\n",
      "(599, 0.11539021)\n",
      "(418, 0.11528808)\n",
      "(942, 0.115021855)\n",
      "(336, 0.11500651)\n",
      "(138, 0.11480793)\n",
      "(932, 0.11452961)\n",
      "(729, 0.11441955)\n",
      "(448, 0.11436227)\n",
      "(567, 0.11434406)\n",
      "(389, 0.11420995)\n",
      "(666, 0.114073485)\n",
      "(880, 0.11401573)\n",
      "(475, 0.11376795)\n",
      "(426, 0.11353618)\n",
      "(546, 0.113532215)\n",
      "(769, 0.113446385)\n",
      "(450, 0.11344218)\n",
      "(339, 0.11303547)\n",
      "(309, 0.1130214)\n",
      "(422, 0.11290169)\n",
      "(846, 0.112890154)\n",
      "(376, 0.11279854)\n",
      "(194, 0.112703204)\n",
      "(902, 0.112583846)\n",
      "(820, 0.11256775)\n",
      "(182, 0.11254299)\n",
      "(783, 0.11243591)\n",
      "(694, 0.11240265)\n",
      "(233, 0.11238557)\n",
      "(3, 0.112306446)\n",
      "(488, 0.11184871)\n",
      "(380, 0.1117183)\n",
      "(893, 0.11170632)\n",
      "(676, 0.11168891)\n",
      "(545, 0.11137736)\n",
      "(124, 0.11132741)\n",
      "(972, 0.11106023)\n",
      "(499, 0.11075848)\n",
      "(451, 0.11055803)\n",
      "(667, 0.11035168)\n",
      "(997, 0.11032882)\n",
      "(813, 0.10983986)\n",
      "(848, 0.10959357)\n",
      "(999, 0.10942641)\n",
      "(849, 0.10926449)\n",
      "(337, 0.108829945)\n",
      "(506, 0.10880202)\n",
      "(989, 0.10856706)\n",
      "(814, 0.1083667)\n",
      "(764, 0.10768342)\n",
      "(644, 0.1076487)\n",
      "(276, 0.107411325)\n",
      "(863, 0.107340544)\n",
      "(662, 0.1072284)\n",
      "(636, 0.1071119)\n",
      "(952, 0.10670245)\n",
      "(870, 0.10612196)\n",
      "(508, 0.10599616)\n",
      "(294, 0.10542646)\n",
      "(807, 0.10531497)\n",
      "(219, 0.10524392)\n",
      "(974, 0.105169535)\n",
      "(367, 0.105145425)\n",
      "(860, 0.10511094)\n",
      "(741, 0.10508403)\n",
      "(307, 0.10459396)\n",
      "(836, 0.10372743)\n",
      "(279, 0.103428185)\n",
      "(405, 0.10231432)\n",
      "(1016, 0.1017991)\n",
      "(531, 0.101588905)\n",
      "(431, 0.10154778)\n",
      "(701, 0.10084018)\n",
      "(702, 0.10077223)\n",
      "(658, 0.100697905)\n",
      "(798, 0.10016167)\n",
      "(878, 0.099965304)\n",
      "(709, 0.099553525)\n",
      "(300, 0.09914622)\n",
      "(674, 0.09867951)\n",
      "(313, 0.09842712)\n",
      "(372, 0.098315805)\n",
      "(527, 0.09825456)\n",
      "(793, 0.09781501)\n",
      "(593, 0.09729418)\n",
      "(271, 0.09637615)\n",
      "(7, 0.09636611)\n",
      "(765, 0.09635088)\n",
      "(539, 0.0962421)\n",
      "(897, 0.09619886)\n",
      "(839, 0.09576869)\n",
      "(536, 0.09566924)\n",
      "(523, 0.09553781)\n",
      "(36, 0.09506947)\n",
      "(822, 0.09462458)\n",
      "(314, 0.093500584)\n",
      "(392, 0.09341729)\n",
      "(865, 0.09283176)\n",
      "(298, 0.09270963)\n",
      "(137, 0.09261918)\n",
      "(984, 0.09166953)\n",
      "(540, 0.09023866)\n",
      "(129, 0.090155065)\n",
      "(587, 0.08982113)\n",
      "(53, 0.089790374)\n",
      "(110, 0.0893012)\n",
      "(692, 0.08925286)\n",
      "(447, 0.08815858)\n",
      "(1014, 0.08697173)\n",
      "(402, 0.08618492)\n",
      "(937, 0.084546626)\n",
      "(582, 0.08378801)\n",
      "(84, 0.08374372)\n",
      "(65, 0.07912564)\n",
      "(443, 0.0768753)\n",
      "(835, 0.074851334)\n",
      "(591, 0.071704924)\n",
      "(96, 0.07007399)\n",
      "(927, 0.06973681)\n",
      "(126, 0.06871936)\n",
      "(221, 0.06823686)\n",
      "(236, 0.06794059)\n",
      "(41, 0.064780414)\n",
      "(356, 0.06329107)\n",
      "(869, 0.063049376)\n",
      "(1006, 0.059055954)\n",
      "(627, 0.057290524)\n",
      "(414, 0.05728507)\n",
      "(214, 0.056802094)\n",
      "(100, 0.049660265)\n",
      "(654, 0.04830876)\n",
      "(82, 0.047549963)\n",
      "(195, 0.046288967)\n",
      "(961, 0.04205802)\n",
      "(390, 0.037670583)\n",
      "(693, 0.036950707)\n",
      "(68, 0.019956768)\n",
      "(4, 0.01042065)\n",
      "(56, 0.0062648654)\n"
     ]
    }
   ],
   "source": [
    "prob = model.get_gates(mode = 'prob')\n",
    "prob_filtered = []\n",
    "for i, x in enumerate(prob):\n",
    "    prob_filtered.append((i, x))\n",
    "\n",
    "    \n",
    "prob_filtered.sort(key = lambda x: x[1], reverse=True)\n",
    "print(len(prob_filtered))\n",
    "print(*prob_filtered, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "(43, 0.8595033)\n",
      "(317, 0.7928253)\n",
      "(385, 0.7583463)\n",
      "(512, 0.7189029)\n",
      "(620, 0.6643486)\n",
      "(766, 0.6528963)\n",
      "(509, 0.5164768)\n",
      "(663, 0.5105725)\n",
      "(628, 0.5030268)\n",
      "(262, 0.46868357)\n",
      "(490, 0.45932406)\n",
      "(866, 0.37786764)\n",
      "(73, 0.37303284)\n",
      "(716, 0.36529917)\n",
      "(436, 0.34358597)\n",
      "(664, 0.310651)\n",
      "(633, 0.2933525)\n",
      "(922, 0.2923891)\n",
      "(960, 0.2917078)\n",
      "(99, 0.2598001)\n",
      "(890, 0.24981438)\n",
      "(410, 0.23403552)\n",
      "(77, 0.20041768)\n",
      "(26, 0.19604516)\n",
      "(1001, 0.15572034)\n",
      "(399, 0.1446345)\n",
      "(964, 0.084242664)\n",
      "(672, 0.05918716)\n",
      "(38, 0.04006384)\n",
      "(774, 0.034799322)\n",
      "(962, 0.032568675)\n",
      "(465, 0.0055365954)\n",
      "(54, 0.002004441)\n"
     ]
    }
   ],
   "source": [
    "raw = model.get_gates(mode = 'raw')\n",
    "raw_filtered = []\n",
    "for i, x in enumerate(raw):\n",
    "    if x>0: raw_filtered.append((i, x))\n",
    "\n",
    "raw_filtered.sort(key = lambda x: x[1], reverse=True)\n",
    "print(len(raw_filtered))\n",
    "print(*raw_filtered, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "'''\n",
    "Developed by Abdullah Al Mamun\n",
    "- Feature selection framework \n",
    "- Date: Feb 2019\n",
    "'''\n",
    "\n",
    "# Load pkgses\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# Masrur confusion matrix\n",
    "import collections\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pylab import savefig\n",
    "\n",
    "# STG comment if local\n",
    "# from stg import STG\n",
    "# import scipy.stats # for creating a simple dataset \n",
    "# import torch\n",
    "\n",
    "#keep comment otherwise fig will be distorted\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "# import seaborn as sns; sns.set(color_codes=True)\n",
    "# from pylab import savefig\n",
    "\n",
    "# PATH = '/lclhome/mmamu009/FIU/PhD/Feature-Selection/Type-classification/lncRNA_CAE_Ext/'\n",
    "PATH = '/Users/mamun/FIU/PhD/Feature_selection/Type_classification/lncRNA_CAE_Ext/Experiments/'\n",
    "# PATH = '/Users/mamun/FIU/PhD/Feature_selection/Type_classification/lncRNA_CAE_Ext/Results/Validation_RBT/'\n",
    "# fileName = 'tumor/TCGA_33_htseq_fpkm_sorted_lncRNA.csv'\n",
    "# fileName = 'tumor/groups/TCGA_g1_htseq_fpkm_sorted_lncRNA.csv'\n",
    "fileName = 'tumor/Example-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000005206.15</th>\n",
       "      <th>ENSG00000083622.8</th>\n",
       "      <th>ENSG00000088970.14</th>\n",
       "      <th>ENSG00000099869.7</th>\n",
       "      <th>ENSG00000100181.20</th>\n",
       "      <th>ENSG00000104691.13</th>\n",
       "      <th>ENSG00000115934.11</th>\n",
       "      <th>ENSG00000117242.7</th>\n",
       "      <th>ENSG00000118412.11</th>\n",
       "      <th>ENSG00000122043.9</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000225138.6</th>\n",
       "      <th>ENSG00000225140.1</th>\n",
       "      <th>ENSG00000225144.2</th>\n",
       "      <th>ENSG00000225146.1</th>\n",
       "      <th>ENSG00000225148.1</th>\n",
       "      <th>ENSG00000225152.1</th>\n",
       "      <th>ENSG00000225156.2</th>\n",
       "      <th>ENSG00000225163.3</th>\n",
       "      <th>ENSG00000225166.1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.474815</td>\n",
       "      <td>1.015497</td>\n",
       "      <td>3.938578</td>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.217501</td>\n",
       "      <td>2.445204</td>\n",
       "      <td>0.024322</td>\n",
       "      <td>1.651243</td>\n",
       "      <td>1.495478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.775220</td>\n",
       "      <td>0.038819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14878</td>\n",
       "      <td>0.01209</td>\n",
       "      <td>0.265130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.313495</td>\n",
       "      <td>0.608207</td>\n",
       "      <td>2.179453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015612</td>\n",
       "      <td>1.441505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938922</td>\n",
       "      <td>1.537811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.017635</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07404</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.257730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.396907</td>\n",
       "      <td>1.475637</td>\n",
       "      <td>2.591849</td>\n",
       "      <td>0.016740</td>\n",
       "      <td>0.023794</td>\n",
       "      <td>2.690200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.583583</td>\n",
       "      <td>1.415676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.744041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.377362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.960528</td>\n",
       "      <td>1.142912</td>\n",
       "      <td>2.312048</td>\n",
       "      <td>0.010347</td>\n",
       "      <td>0.043719</td>\n",
       "      <td>2.203999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965548</td>\n",
       "      <td>1.920312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.571482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.280320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.296875</td>\n",
       "      <td>0.447136</td>\n",
       "      <td>2.104808</td>\n",
       "      <td>0.162264</td>\n",
       "      <td>0.178781</td>\n",
       "      <td>1.747109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762944</td>\n",
       "      <td>1.715874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.415390</td>\n",
       "      <td>0.132493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.686951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000005206.15  ENSG00000083622.8  ENSG00000088970.14  \\\n",
       "0            4.474815           1.015497            3.938578   \n",
       "1            4.313495           0.608207            2.179453   \n",
       "2            4.396907           1.475637            2.591849   \n",
       "3            3.960528           1.142912            2.312048   \n",
       "4            4.296875           0.447136            2.104808   \n",
       "\n",
       "   ENSG00000099869.7  ENSG00000100181.20  ENSG00000104691.13  \\\n",
       "0           0.011318            0.217501            2.445204   \n",
       "1           0.000000            0.015612            1.441505   \n",
       "2           0.016740            0.023794            2.690200   \n",
       "3           0.010347            0.043719            2.203999   \n",
       "4           0.162264            0.178781            1.747109   \n",
       "\n",
       "   ENSG00000115934.11  ENSG00000117242.7  ENSG00000118412.11  \\\n",
       "0            0.024322           1.651243            1.495478   \n",
       "1            0.000000           0.938922            1.537811   \n",
       "2            0.000000           1.583583            1.415676   \n",
       "3            0.000000           0.965548            1.920312   \n",
       "4            0.000000           0.762944            1.715874   \n",
       "\n",
       "   ENSG00000122043.9  ...  ENSG00000225138.6  ENSG00000225140.1  \\\n",
       "0                0.0  ...           2.775220           0.038819   \n",
       "1                0.0  ...           5.017635           0.018949   \n",
       "2                0.0  ...           3.744041           0.000000   \n",
       "3                0.0  ...           4.571482           0.000000   \n",
       "4                0.0  ...           2.415390           0.132493   \n",
       "\n",
       "   ENSG00000225144.2  ENSG00000225146.1  ENSG00000225148.1  ENSG00000225152.1  \\\n",
       "0                0.0           0.059680                0.0            0.14878   \n",
       "1                0.0           0.057897                0.0            0.07404   \n",
       "2                0.0           0.000000                0.0            0.00000   \n",
       "3                0.0           0.107283                0.0            0.00000   \n",
       "4                0.0           0.117850                0.0            0.00000   \n",
       "\n",
       "   ENSG00000225156.2  ENSG00000225163.3  ENSG00000225166.1  target  \n",
       "0            0.01209           0.265130                0.0       0  \n",
       "1            0.00000           0.257730                0.0       0  \n",
       "2            0.00000           0.377362                0.0       0  \n",
       "3            0.00000           1.280320                0.0       0  \n",
       "4            0.00000           0.686951                0.0       0  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "#     loading data, minMax normalization and train tesing split\n",
    "df = pd.read_csv(PATH + fileName)\n",
    "df.fillna(0.0)\n",
    "# df = df.replace(np.nan,0.0)\n",
    "X = df.iloc[:,1:-1]\n",
    "y = df.iloc[:,-1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 1022)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "cls = y.unique()\n",
    "len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 1022) (40, 1022)\n",
      "(127, 1022) (32, 1022)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify = y, random_state = 1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8, stratify = y_train, random_state = 1)\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. utils.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import six\n",
    "import collections\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "\n",
    "SKIP_TYPES = six.string_types\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    '''\n",
    "    Assuming X and y are numpy arrays and \n",
    "     with X.shape = (n_samples, n_features) \n",
    "          y.shape = (n_samples,)\n",
    "    '''\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.X[i]\n",
    "        data = np.array(data).astype(np.float32)\n",
    "        if self.y is not None:\n",
    "            return dict(input=data, label=self.y[i])\n",
    "        else:\n",
    "            return dict(input=data)\n",
    "\n",
    "\n",
    "class FastTensorDataLoader:\n",
    "    \"\"\"\n",
    "    A DataLoader-like object for a set of tensors that can be much faster than\n",
    "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
    "    the dataset and calls cat (slow).\n",
    "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
    "    \"\"\"\n",
    "    def __init__(self, *tensors, tensor_names, batch_size=32, shuffle=False):\n",
    "        \"\"\"\n",
    "        Initialize a FastTensorDataLoader.\n",
    "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
    "        :param tensor_names: name of tensors (for feed_dict)\n",
    "        :param batch_size: batch size to load.\n",
    "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
    "            iterator is created out of this object.\n",
    "        :returns: A FastTensorDataLoader.\n",
    "        \"\"\"\n",
    "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.tensor_names = tensor_names\n",
    "\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Calculate # batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder > 0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            r = torch.randperm(self.dataset_len)\n",
    "            self.tensors = [t[r] for t in self.tensors]\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        batch = {}\n",
    "        for k in range(len(self.tensor_names)):\n",
    "            batch.update({self.tensor_names[k]: self.tensors[k][self.i:self.i+self.batch_size]})\n",
    "        self.i += self.batch_size\n",
    "        return batch\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "\n",
    "'''standardize_dataset function is from utils_jared.py'''\n",
    "def standardize_dataset(dataset, offset, scale):\n",
    "    norm_ds = copy.deepcopy(dataset)\n",
    "    norm_ds['x'] = (norm_ds['x'] - offset) / scale\n",
    "    return norm_ds\n",
    "\n",
    "\n",
    "'''load_datasets function is from utils_jared.py'''\n",
    "def load_datasets(dataset_file):\n",
    "    datasets = defaultdict(dict)\n",
    "    with h5py.File(dataset_file, 'r') as fp:\n",
    "        for ds in fp:\n",
    "            for array in fp[ds]:\n",
    "                datasets[ds][array] = fp[ds][array][:]\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def load_cox_gaussian_data():\n",
    "    dataset_file = os.path.join(os.path.dirname(__file__), \n",
    "        'datasets/gaussian_survival_data.h5')\n",
    "    datasets = defaultdict(dict)\n",
    "    with h5py.File(dataset_file, 'r') as fp:\n",
    "        for ds in fp:\n",
    "            for array in fp[ds]:\n",
    "                datasets[ds][array] = fp[ds][array][:]\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def prepare_data(x, label):\n",
    "    if isinstance(label, dict):\n",
    "       e, t = label['e'], label['t']\n",
    "\n",
    "    # Sort training data for accurate partial likelihood calculation.\n",
    "    sort_idx = np.argsort(t)[::-1]\n",
    "    x = x[sort_idx]\n",
    "    e = e[sort_idx]\n",
    "    t = t[sort_idx]\n",
    "\n",
    "    #return x, {'e': e, 't': t} this is for parse_data(x, label); see the third line in the parse_data function. \n",
    "    return {'x': x, 'e': e, 't': t}\n",
    "\n",
    "def probe_infnan(v, name, extras={}):\n",
    "    nps = torch.isnan(v)\n",
    "    s = nps.sum().item()\n",
    "    if s > 0:\n",
    "        print('>>> {} >>>'.format(name))\n",
    "        print(name, s)\n",
    "        print(v[nps])\n",
    "        for k, val in extras.items():\n",
    "            print(k, val, val.sum().item())\n",
    "        quit()\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, *args):\n",
    "        if len(args) == 1:\n",
    "            return args[0]\n",
    "        return args\n",
    "\n",
    "def get_batcnnorm(bn, nr_features=None, nr_dims=1):\n",
    "    if isinstance(bn, nn.Module):\n",
    "        return bn\n",
    "\n",
    "    assert 1 <= nr_dims <= 3\n",
    "\n",
    "    if bn in (True, 'async'):\n",
    "        clz_name = 'BatchNorm{}d'.format(nr_dims)\n",
    "        return getattr(nn, clz_name)(nr_features)\n",
    "    else:\n",
    "        raise ValueError('Unknown type of batch normalization: {}.'.format(bn))\n",
    "\n",
    "\n",
    "def get_dropout(dropout, nr_dims=1):\n",
    "    if isinstance(dropout, nn.Module):\n",
    "        return dropout\n",
    "\n",
    "    if dropout is True:\n",
    "        dropout = 0.5\n",
    "    if nr_dims == 1:\n",
    "        return nn.Dropout(dropout, True)\n",
    "    else:\n",
    "        clz_name = 'Dropout{}d'.format(nr_dims)\n",
    "        return getattr(nn, clz_name)(dropout)\n",
    "\n",
    "\n",
    "def get_activation(act):\n",
    "    if isinstance(act, nn.Module):\n",
    "        return act\n",
    "\n",
    "    assert type(act) is str, 'Unknown type of activation: {}.'.format(act)\n",
    "    act_lower = act.lower()\n",
    "    if act_lower == 'identity':\n",
    "        return Identity()\n",
    "    elif act_lower == 'relu':\n",
    "        return nn.ReLU(True)\n",
    "    elif act_lower == 'selu':\n",
    "        return nn.SELU(True)\n",
    "    elif act_lower == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif act_lower == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        try:\n",
    "            return getattr(nn, act)\n",
    "        except AttributeError:\n",
    "            raise ValueError('Unknown activation function: {}.'.format(act))\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer, model, *args, **kwargs):\n",
    "    if isinstance(optimizer, (optim.Optimizer)):\n",
    "        return optimizer\n",
    "\n",
    "    if type(optimizer) is str:\n",
    "        try:\n",
    "            optimizer = getattr(optim, optimizer)\n",
    "        except AttributeError:\n",
    "            raise ValueError('Unknown optimizer type: {}.'.format(optimizer))\n",
    "    return optimizer(filter(lambda p: p.requires_grad, model.parameters()), *args, **kwargs)\n",
    "    \n",
    "\n",
    "def stmap(func, iterable):\n",
    "    if isinstance(iterable, six.string_types):\n",
    "        return func(iterable)\n",
    "    elif isinstance(iterable, (collections.Sequence, collections.UserList)):\n",
    "        return [stmap(func, v) for v in iterable]\n",
    "    elif isinstance(iterable, collections.Set):\n",
    "        return {stmap(func, v) for v in iterable}\n",
    "    elif isinstance(iterable, (collections.Mapping, collections.UserDict)):\n",
    "        return {k: stmap(func, v) for k, v in iterable.items()}\n",
    "    else:\n",
    "        return func(iterable)\n",
    "\n",
    "\n",
    "def _as_tensor(o):\n",
    "    from torch.autograd import Variable\n",
    "    if isinstance(o, SKIP_TYPES):\n",
    "        return o\n",
    "    if isinstance(o, Variable):\n",
    "        return o\n",
    "    if torch.is_tensor(o):\n",
    "        return o\n",
    "    return torch.from_numpy(np.array(o))\n",
    "\n",
    "\n",
    "def as_tensor(obj):\n",
    "    return stmap(_as_tensor, obj)\n",
    "\n",
    "\n",
    "def _as_numpy(o):\n",
    "    from torch.autograd import Variable\n",
    "    if isinstance(o, SKIP_TYPES):\n",
    "        return o\n",
    "    if isinstance(o, Variable):\n",
    "        o = o\n",
    "    if torch.is_tensor(o):\n",
    "        return o.cpu().numpy()\n",
    "    return np.array(o)\n",
    "\n",
    "\n",
    "def as_numpy(obj):\n",
    "    return stmap(_as_numpy, obj)\n",
    "\n",
    "\n",
    "def _as_float(o):\n",
    "    if isinstance(o, SKIP_TYPES):\n",
    "        return o\n",
    "    if torch.is_tensor(o):\n",
    "        return o.item()\n",
    "    arr = as_numpy(o)\n",
    "    assert arr.size == 1\n",
    "    return float(arr)\n",
    "\n",
    "\n",
    "def as_float(obj):\n",
    "    return stmap(_as_float, obj)\n",
    "\n",
    "\n",
    "def _as_cpu(o):\n",
    "    from torch.autograd import Variable\n",
    "    if isinstance(o, Variable) or torch.is_tensor(o):\n",
    "        return o.cpu()\n",
    "    return o\n",
    "\n",
    "\n",
    "def as_cpu(obj):\n",
    "    return stmap(_as_cpu, obj)\n",
    "\n",
    "\n",
    "## For synthetic dataset creation\n",
    "import math\n",
    "from sklearn.datasets import make_moons\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# Create a simple dataset\n",
    "def create_twomoon_dataset(n, p):\n",
    "    relevant, y = make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)\n",
    "    print(y.shape)\n",
    "    noise_vector = norm.rvs(loc=0, scale=1, size=[n,p-2])\n",
    "    data = np.concatenate([relevant, noise_vector], axis=1)\n",
    "    print(data.shape)\n",
    "    return data, y\n",
    "\n",
    "\n",
    "def create_sin_dataset(n,p):\n",
    "    x1=5*(np.random.uniform(0,1,n)).reshape(-1,1)\n",
    "    x2=5*(np.random.uniform(0,1,n)).reshape(-1,1)\n",
    "    y=np.sin(x1)*np.cos(x2)**3\n",
    "    relevant=np.hstack((x1,x2))\n",
    "    noise_vector = norm.rvs(loc=0, scale=1, size=[n,p-2])\n",
    "    data = np.concatenate([relevant, noise_vector], axis=1)\n",
    "    return data, y.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def create_simple_sin_dataset(n, p):\n",
    "    '''This dataset was added to provide an example of L1 norm reg failure for presentation.\n",
    "    '''\n",
    "    assert p == 2\n",
    "    x1 = np.random.uniform(-math.pi, math.pi, n).reshape(n ,1)\n",
    "    x2 = np.random.uniform(-math.pi, math.pi, n).reshape(n, 1)\n",
    "    y = np.sin(x1)\n",
    "    data = np.concatenate([x1, x2], axis=1)\n",
    "    print(\"data.shape: {}\".format(data.shape))\n",
    "    return data, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. layers.py\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "__all__ = [\n",
    "    'LinearLayer', 'MLPLayer', 'FeatureSelector',\n",
    "]\n",
    "\n",
    "class FeatureSelector(nn.Module):\n",
    "    def __init__(self, input_dim, sigma, device):\n",
    "        super(FeatureSelector, self).__init__()\n",
    "        self.mu = torch.nn.Parameter(0.01*torch.randn(input_dim, ), requires_grad=True)\n",
    "        self.noise = torch.randn(self.mu.size()) \n",
    "        self.sigma = sigma\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, prev_x):\n",
    "        z = self.mu + self.sigma*self.noise.normal_()*self.training \n",
    "        stochastic_gate = self.hard_sigmoid(z)\n",
    "        new_x = prev_x * stochastic_gate\n",
    "        return new_x\n",
    "    \n",
    "    def hard_sigmoid(self, x):\n",
    "        return torch.clamp(x+0.5, 0.0, 1.0)\n",
    "\n",
    "    def regularizer(self, x):\n",
    "        ''' Gaussian CDF. '''\n",
    "        return 0.5 * (1 + torch.erf(x / math.sqrt(2))) \n",
    "\n",
    "    def _apply(self, fn):\n",
    "        super(FeatureSelector, self)._apply(fn)\n",
    "        self.noise = fn(self.noise)\n",
    "        return self\n",
    "\n",
    "class LinearLayer(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, batch_norm=None, dropout=None, bias=None, activation=None):\n",
    "        if bias is None:\n",
    "            bias = (batch_norm is None)\n",
    "\n",
    "        modules = [nn.Linear(in_features, out_features, bias=bias)]\n",
    "        if batch_norm is not None and batch_norm is not False:\n",
    "            modules.append(get_batcnnorm(batch_norm, out_features, 1))\n",
    "        if dropout is not None and dropout is not False:\n",
    "            modules.append(get_dropout(dropout, 1))\n",
    "        if activation is not None and activation is not False:\n",
    "            modules.append(get_activation(activation))\n",
    "        super().__init__(*modules)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.reset_parameters()\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, batch_norm=None, dropout=None, activation='relu', flatten=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = []\n",
    "        elif type(hidden_dims) is int:\n",
    "            hidden_dims = [hidden_dims]\n",
    "\n",
    "        dims = [input_dim]\n",
    "        dims.extend(hidden_dims)\n",
    "        dims.append(output_dim)\n",
    "        modules = []\n",
    "\n",
    "        nr_hiddens = len(hidden_dims)\n",
    "        for i in range(nr_hiddens):\n",
    "            layer = LinearLayer(dims[i], dims[i+1], batch_norm=batch_norm, dropout=dropout, activation=activation)\n",
    "            modules.append(layer)\n",
    "        layer = nn.Linear(dims[-2], dims[-1], bias=True)\n",
    "        modules.append(layer)\n",
    "        self.mlp = nn.Sequential(*modules)\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.reset_parameters()\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.flatten:\n",
    "            input = input.view(input.size(0), -1)\n",
    "        return self.mlp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# from .layers import MLPLayer, FeatureSelector, GatingLayer\n",
    "# from .losses import PartialLogLikelihood\n",
    "\n",
    "__all__ = ['MLPModel', 'MLPRegressionModel', 'MLPClassificationModel', 'LinearRegressionModel', 'LinearClassificationModel']\n",
    "\n",
    "\n",
    "class ModelIOKeysMixin(object):\n",
    "    def _get_input(self, feed_dict):\n",
    "        return feed_dict['input']\n",
    "\n",
    "    def _get_label(self, feed_dict):\n",
    "        return feed_dict['label']\n",
    "\n",
    "    def _get_covariate(self, feed_dict):\n",
    "        '''For cox'''\n",
    "        return feed_dict['X']\n",
    "\n",
    "    def _get_fail_indicator(self, feed_dict):\n",
    "        '''For cox'''\n",
    "        return feed_dict['E'].reshape(-1, 1)\n",
    "\n",
    "    def _get_failure_time(self, feed_dict):\n",
    "        '''For cox'''\n",
    "        return feed_dict['T']\n",
    "\n",
    "    def _compose_output(self, value):\n",
    "        return dict(pred=value)\n",
    "\n",
    "\n",
    "class MLPModel(MLPLayer):\n",
    "    def freeze_weights(self):\n",
    "        for name, p in self.named_parameters():\n",
    "            if name != 'mu':\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def get_gates(self, mode):\n",
    "        if mode == 'raw':\n",
    "            return self.mu.detach().cpu().numpy()\n",
    "        elif mode == 'prob':\n",
    "            return np.minimum(1.0, np.maximum(0.0, self.mu.detach().cpu().numpy() + 0.5)) \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "class STGReconstructionModel(MLPModel, ModelIOKeysMixin):\n",
    "    def __init__(self, input_dim, nr_classes, hidden_dims, device, batch_norm=None, dropout=None, activation='relu',\n",
    "                 sigma=1.0, lam=0.1):\n",
    "        super().__init__(input_dim, nr_classes, hidden_dims,\n",
    "                         batch_norm=batch_norm, dropout=dropout, activation=activation)\n",
    "        self.FeatureSelector = FeatureSelector(input_dim, sigma, device)\n",
    "#         self.softmax = nn.Softmax()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.reg = self.FeatureSelector.regularizer\n",
    "        self.lam = lam \n",
    "        self.mu = self.FeatureSelector.mu\n",
    "        self.sigma = self.FeatureSelector.sigma\n",
    "        \n",
    "    def forward(self, feed_dict):\n",
    "        x = self.FeatureSelector(self._get_input(feed_dict))\n",
    "        pred = super().forward(x)\n",
    "        if self.training:\n",
    "            loss = self.loss(pred, self._get_label(feed_dict))\n",
    "            reg = torch.mean(self.reg((self.mu + 0.5)/self.sigma)) \n",
    "            total_loss = loss + self.lam * reg \n",
    "            return total_loss, dict(), dict() \n",
    "        else:\n",
    "            return self._compose_output(pred)\n",
    "            \n",
    "\n",
    "class STGClassificationModel(MLPModel, ModelIOKeysMixin):\n",
    "    def __init__(self, input_dim, nr_classes, hidden_dims, device, batch_norm=None, dropout=None, activation='relu',\n",
    "                 sigma=1.0, lam=0.1):\n",
    "        super().__init__(input_dim, nr_classes, hidden_dims,\n",
    "                         batch_norm=batch_norm, dropout=dropout, activation=activation)\n",
    "        self.FeatureSelector = FeatureSelector(input_dim, sigma, device)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.reg = self.FeatureSelector.regularizer\n",
    "        self.lam = lam \n",
    "        self.mu = self.FeatureSelector.mu\n",
    "        self.sigma = self.FeatureSelector.sigma\n",
    "        \n",
    "    def forward(self, feed_dict):\n",
    "        x = self.FeatureSelector(self._get_input(feed_dict))\n",
    "        logits = super().forward(x)\n",
    "        if self.training:\n",
    "            loss = self.loss(logits, self._get_label(feed_dict))\n",
    "            reg = torch.mean(self.reg((self.mu + 0.5)/self.sigma)) \n",
    "            total_loss = loss + self.lam * reg \n",
    "            return total_loss, dict(), dict() \n",
    "        else:\n",
    "            return self._compose_output(logits)\n",
    "\n",
    "    def _compose_output(self, logits):\n",
    "        value = self.softmax(logits)\n",
    "        _, pred = value.max(dim=1)\n",
    "        return dict(prob=value, pred=pred, logits=logits)\n",
    "\n",
    "class MLPClassificationModel(MLPModel, ModelIOKeysMixin):\n",
    "    def __init__(self, input_dim, nr_classes, hidden_dims, batch_norm=None, dropout=None, activation='relu'):\n",
    "        super().__init__(input_dim, nr_classes, hidden_dims,\n",
    "                         batch_norm=batch_norm, dropout=dropout, activation=activation)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, feed_dict):\n",
    "        logits = super().forward(self._get_input(feed_dict))\n",
    "        if self.training:\n",
    "            loss = self.loss(logits, self._get_label(feed_dict))\n",
    "            return loss, dict(), dict()\n",
    "        else:\n",
    "            return self._compose_output(logits)\n",
    "\n",
    "    def _compose_output(self, logits):\n",
    "        value = self.softmax(logits)\n",
    "        _, pred = value.max(dim=1)\n",
    "        return dict(prob=value, pred=pred, logits=logits)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meter.py\n",
    "import six\n",
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "\n",
    "def map_exec(func, *iterables):\n",
    "    return list(map(func, *iterables))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    val = 0\n",
    "    avg = 0\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    tot_count = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.tot_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.tot_count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class GroupMeters(object):\n",
    "    def __init__(self):\n",
    "        self._meters = collections.defaultdict(AverageMeter)\n",
    "\n",
    "    def reset(self):\n",
    "        map_exec(AverageMeter.reset, self._meters.values())\n",
    "\n",
    "    def update(self, updates=None, value=None, n=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "            >>> meters.update(key, value)\n",
    "            >>> meters.update({key1: value1, key2: value2})\n",
    "            >>> meters.update(key1=value1, key2=value2)\n",
    "        \"\"\"\n",
    "        if updates is None:\n",
    "            updates = {}\n",
    "        if updates is not None and value is not None:\n",
    "            updates = {updates: value}\n",
    "        updates.update(kwargs)\n",
    "        for k, v in updates.items():\n",
    "            self._meters[k].update(v, n=n)\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self._meters[name]\n",
    "\n",
    "    def items(self):\n",
    "        return self._meters.items()\n",
    "\n",
    "    @property\n",
    "    def sum(self):\n",
    "        return {k: m.sum for k, m in self._meters.items() if m.count > 0}\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return {k: m.avg for k, m in self._meters.items() if m.count > 0}\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return {k: m.val for k, m in self._meters.items() if m.count > 0}\n",
    "\n",
    "    def format(self, caption, values, kv_format, glue):\n",
    "        meters_kv = self._canonize_values(values)\n",
    "        log_str = [caption]\n",
    "        log_str.extend(itertools.starmap(kv_format.format, sorted(meters_kv.items())))\n",
    "        return glue.join(log_str)\n",
    "\n",
    "    def format_simple(self, caption, values='avg', compressed=True):\n",
    "        if compressed:\n",
    "            return self.format(caption, values, '{}={:4f}', ' ')\n",
    "        else:\n",
    "            return self.format(caption, values, '\\t{} = {:4f}', '\\n')\n",
    "\n",
    "    def dump(self, filename, values='avg'):\n",
    "        meters_kv = self._canonize_values(values)\n",
    "        with open(filename, 'a') as f:\n",
    "            #f.write(io.dumps_json(meters_kv, compressed=False))\n",
    "            f.write(json.dumps(meters_kv, cls=JsonObjectEncoder, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "            f.write('\\n')\n",
    "\n",
    "    def _canonize_values(self, values):\n",
    "        if isinstance(values, six.string_types):\n",
    "            assert values in ('avg', 'val', 'sum')\n",
    "            meters_kv = getattr(self, values)\n",
    "        else:\n",
    "            meters_kv = values\n",
    "        return meters_kv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stg.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import logging.config \n",
    "import os.path as osp\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger(\"my-logger\")\n",
    "\n",
    "\n",
    "__all__ = ['STG']\n",
    "\n",
    "\n",
    "def _standard_truncnorm_sample(lower_bound, upper_bound, sample_shape=torch.Size()):\n",
    "    r\"\"\"\n",
    "    Implements accept-reject algorithm for doubly truncated standard normal distribution.\n",
    "    (Section 2.2. Two-sided truncated normal distribution in [1])\n",
    "    [1] Robert, Christian P. \"Simulation of truncated normal variables.\" Statistics and computing 5.2 (1995): 121-125.\n",
    "    Available online: https://arxiv.org/abs/0907.4010\n",
    "    Args:\n",
    "        lower_bound (Tensor): lower bound for standard normal distribution. Best to keep it greater than -4.0 for\n",
    "        stable results\n",
    "        upper_bound (Tensor): upper bound for standard normal distribution. Best to keep it smaller than 4.0 for\n",
    "        stable results\n",
    "    \"\"\"\n",
    "    x = torch.randn(sample_shape)\n",
    "    done = torch.zeros(sample_shape).byte() \n",
    "    while not done.all():\n",
    "        proposed_x = lower_bound + torch.rand(sample_shape) * (upper_bound - lower_bound)\n",
    "        if (upper_bound * lower_bound).lt(0.0):  # of opposite sign\n",
    "            log_prob_accept = -0.5 * proposed_x**2\n",
    "        elif upper_bound < 0.0:  # both negative\n",
    "            log_prob_accept = 0.5 * (upper_bound**2 - proposed_x**2)\n",
    "        else:  # both positive\n",
    "            assert(lower_bound.gt(0.0))\n",
    "            log_prob_accept = 0.5 * (lower_bound**2 - proposed_x**2)\n",
    "        prob_accept = torch.exp(log_prob_accept).clamp_(0.0, 1.0)\n",
    "        accept = torch.bernoulli(prob_accept).byte() & ~done\n",
    "        if accept.any():\n",
    "            accept = accept.bool()\n",
    "            x[accept] = proposed_x[accept]\n",
    "            accept = accept.byte()\n",
    "            done |= accept\n",
    "    return x\n",
    "\n",
    "\n",
    "class STG(object):\n",
    "    def __init__(self, device, input_dim=784, output_dim=10, hidden_dims=[400, 200], activation='relu', sigma=0.5, lam=0.1,\n",
    "                optimizer='Adam', learning_rate=1e-5,  batch_size=100, freeze_onward=None, feature_selection=True, weight_decay=1e-3, \n",
    "                task_type='classification', report_maps=False, random_state=1, extra_args=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.device = self.get_device(device)\n",
    "        self.report_maps = report_maps \n",
    "        self.task_type = task_type\n",
    "        self.extra_args = extra_args\n",
    "        self.freeze_onward = freeze_onward\n",
    "        self._model = self.build_model(input_dim, output_dim, hidden_dims, activation, sigma, lam, \n",
    "                                       task_type, feature_selection)\n",
    "        self._model.apply(self.init_weights)\n",
    "        self._model = self._model.to(device)\n",
    "        self._optimizer = get_optimizer(optimizer, self._model, lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    def get_device(self, device):\n",
    "        if device == \"cpu\":\n",
    "            device = torch.device(\"cpu\")\n",
    "        elif device == None:\n",
    "            args_cuda = torch.cuda.is_available()\n",
    "            device = device = torch.device(\"cuda\" if args_cuda else \"cpu\")\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only 'cpu' or 'cuda' is a valid option.\")\n",
    "        return device\n",
    "        \n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            stddev = torch.tensor(0.1)\n",
    "            shape = m.weight.shape\n",
    "            m.weight = nn.Parameter(_standard_truncnorm_sample(lower_bound=-2*stddev, upper_bound=2*stddev, \n",
    "                                  sample_shape=shape))\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def build_model(self, input_dim, output_dim, hidden_dims, activation, sigma, lam, task_type, feature_selection):\n",
    "        if task_type == 'classification':\n",
    "            self.metric = nn.CrossEntropyLoss()\n",
    "            self.tensor_names = ('input','label')\n",
    "            return STGClassificationModel(input_dim, output_dim, hidden_dims, device=self.device, activation=activation, sigma=sigma, lam=lam)\n",
    "        if task_type == \"reconstruction\":\n",
    "            self.metric = nn.MSELoss()\n",
    "            self.tensor_names = ('input','label')\n",
    "            return STGReconstructionModel(input_dim, output_dim, hidden_dims, device=self.device, activation=activation, sigma=sigma, lam=lam)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def train_step(self, feed_dict, meters=None):\n",
    "        assert self._model.training\n",
    "\n",
    "        loss, logits, monitors = self._model(feed_dict)\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        #probe_infnan(logits, 'logits')\n",
    "        if self.task_type=='cox':\n",
    "            ci = calc_concordance_index(logits.detach().numpy(), \n",
    "                    feed_dict['E'].detach().numpy(), feed_dict['T'].detach().numpy())\n",
    "        if self.extra_args=='l1-softthresh':\n",
    "            self._model.mlp[0][0].weight.data = self._model.prox_op(self._model.mlp[0][0].weight)\n",
    "\n",
    "        loss = as_float(loss)\n",
    "        if meters is not None:\n",
    "            meters.update(loss=loss)\n",
    "            if self.task_type =='cox':\n",
    "                meters.update(CI=ci)\n",
    "            meters.update(monitors)\n",
    "\n",
    "    def get_dataloader(self, X, y, shuffle):\n",
    "        if self.task_type == 'classification' or self.task_type == 'reconstruction':\n",
    "            data_loader = FastTensorDataLoader(torch.from_numpy(X).float().to(self.device), \n",
    "                        torch.from_numpy(y).long().to(self.device), tensor_names=self.tensor_names,\n",
    "                        batch_size=self.batch_size, shuffle=shuffle)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return data_loader \n",
    "\n",
    "    def fit(self, X, y, nr_epochs, valid_X=None, valid_y=None, \n",
    "        verbose=True, meters=None, early_stop=None, print_interval=1, shuffle=False):\n",
    "        data_loader = self.get_dataloader(X, y, shuffle)\n",
    "\n",
    "        if valid_X is not None:\n",
    "            val_data_loader = self.get_dataloader(valid_X, valid_y, shuffle)\n",
    "        else:\n",
    "            val_data_loader = None\n",
    "        self.train(data_loader, nr_epochs, val_data_loader, verbose, meters, early_stop, print_interval)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        data_loader = self.get_dataloader(X, y, shuffle=None)\n",
    "        meters = GroupMeters()\n",
    "        self.validate(data_loader, self.metric, meters, mode='test')\n",
    "        print(meters.format_simple(''))\n",
    "\n",
    "    def predict(self, X, verbose=True):\n",
    "        dataset = SimpleDataset(X)\n",
    "        data_loader = DataLoader(dataset, batch_size=X.shape[0], shuffle=False) \n",
    "        res = []\n",
    "        self._model.eval()\n",
    "        for feed_dict in data_loader:\n",
    "            feed_dict_np = as_numpy(feed_dict)\n",
    "            feed_dict = as_tensor(feed_dict)\n",
    "            with torch.no_grad():\n",
    "                output_dict = self._model(feed_dict)\n",
    "            output_dict_np = as_numpy(output_dict)\n",
    "            res.append(output_dict_np['pred'])\n",
    "        return np.concatenate(res, axis=0)\n",
    "\n",
    "    def train_epoch(self, data_loader, meters=None):\n",
    "        if meters is None:\n",
    "            meters = GroupMeters()\n",
    "\n",
    "        self._model.train()\n",
    "        end = time.time()\n",
    "        for feed_dict in data_loader:\n",
    "            data_time = time.time() - end; end = time.time()\n",
    "            self.train_step(feed_dict, meters=meters)\n",
    "            step_time = time.time() - end; end = time.time()\n",
    "            #if dev:\n",
    "            #meters.update({'time/data': data_time, 'time/step': step_time})\n",
    "        return meters\n",
    "\n",
    "    def train(self, data_loader, nr_epochs, val_data_loader=None, verbose=True, \n",
    "        meters=None, early_stop=None, print_interval=1):\n",
    "        if meters is None:\n",
    "            meters = GroupMeters()\n",
    "\n",
    "        for epoch in range(1, 1 + nr_epochs):\n",
    "            meters.reset()\n",
    "            if epoch == self.freeze_onward:\n",
    "                self._model.freeze_weights()\n",
    "            self.train_epoch(data_loader, meters=meters)\n",
    "            if verbose and epoch % print_interval == 0:\n",
    "                self.validate(val_data_loader, self.metric, meters)\n",
    "                caption = 'Epoch: {}:'.format(epoch)\n",
    "                print(meters.format_simple(caption))\n",
    "            if early_stop is not None:\n",
    "                flag = early_stop(self._model)\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "    def validate_step(self, feed_dict, metric, meters=None, mode='valid'):\n",
    "        with torch.no_grad():\n",
    "            pred = self._model(feed_dict)\n",
    "        if self.task_type == 'classification':\n",
    "            result = metric(pred['logits'], self._model._get_label(feed_dict))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if meters is not None:\n",
    "            meters.update({mode+'_loss':result})\n",
    "            if self.task_type=='cox':\n",
    "                meters.update({mode+'_CI':val_CI})\n",
    "\n",
    "    def validate(self, data_loader, metric, meters=None, mode='valid'):\n",
    "        if meters is None:\n",
    "            meters = GroupMeters()\n",
    "\n",
    "        self._model.eval()\n",
    "        end = time.time()\n",
    "        for fd in data_loader:\n",
    "            data_time = time.time() - end; end = time.time()\n",
    "            self.validate_step(fd, metric, meters=meters, mode=mode)\n",
    "            step_time = time.time() - end; end = time.time()\n",
    "\n",
    "        return meters.avg\n",
    "\n",
    "    def get_gates(self, mode):\n",
    "        return self._model.get_gates(mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_cuda = torch.cuda.is_available()\n",
    "device = \"cuda\" if args_cuda else \"cpu\"\n",
    "feature_selection = True\n",
    "model = STG(task_type='reconstruction',input_dim=X_train.shape[1], output_dim=2, hidden_dims=[200, 60], activation='relu',\n",
    "    optimizer='Adam', learning_rate=0.01, batch_size=X_train.shape[0], feature_selection=feature_selection, sigma=0.5, lam=0.5, random_state=1, device=device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (127) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-96086d67c0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-7c7718bd5420>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, nr_epochs, valid_X, valid_y, verbose, meters, early_stop, print_interval, shuffle)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mval_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-7c7718bd5420>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, nr_epochs, val_data_loader, verbose, meters, early_stop, print_interval)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_onward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-7c7718bd5420>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, data_loader, meters)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mdata_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mstep_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m#if dev:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-7c7718bd5420>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, feed_dict, meters)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-9a4b4095dccd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feed_dict)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (127) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model.fit(X_train.to_numpy(), y_train.to_numpy(), nr_epochs=1000, valid_X=X_valid.to_numpy(), valid_y=y_valid.to_numpy(), print_interval=200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022\n",
      "(43, 0.8006526)\n",
      "(385, 0.77911294)\n",
      "(766, 0.7016678)\n",
      "(760, 0.6584528)\n",
      "(397, 0.6226771)\n",
      "(317, 0.59799546)\n",
      "(512, 0.5952547)\n",
      "(436, 0.5945548)\n",
      "(509, 0.59013426)\n",
      "(633, 0.5886673)\n",
      "(774, 0.5865657)\n",
      "(890, 0.5839858)\n",
      "(716, 0.57940763)\n",
      "(410, 0.57811105)\n",
      "(799, 0.57642233)\n",
      "(1013, 0.57197356)\n",
      "(962, 0.5714045)\n",
      "(620, 0.55386513)\n",
      "(866, 0.54754704)\n",
      "(196, 0.5449445)\n",
      "(81, 0.544802)\n",
      "(490, 0.54030305)\n",
      "(628, 0.5397509)\n",
      "(54, 0.53692746)\n",
      "(482, 0.53632975)\n",
      "(533, 0.5358793)\n",
      "(454, 0.53491604)\n",
      "(122, 0.5345017)\n",
      "(663, 0.5315797)\n",
      "(510, 0.52963585)\n",
      "(99, 0.5292844)\n",
      "(697, 0.5289945)\n",
      "(714, 0.52882737)\n",
      "(961, 0.5249784)\n",
      "(1001, 0.52460796)\n",
      "(471, 0.5243875)\n",
      "(960, 0.52424777)\n",
      "(272, 0.52322674)\n",
      "(982, 0.5229997)\n",
      "(278, 0.5225135)\n",
      "(671, 0.5224447)\n",
      "(727, 0.5219706)\n",
      "(619, 0.52157676)\n",
      "(922, 0.5195554)\n",
      "(258, 0.51938033)\n",
      "(247, 0.5185374)\n",
      "(555, 0.51829666)\n",
      "(784, 0.5163551)\n",
      "(747, 0.51562953)\n",
      "(37, 0.51544505)\n",
      "(393, 0.5153887)\n",
      "(492, 0.5152874)\n",
      "(719, 0.51472235)\n",
      "(113, 0.5146589)\n",
      "(59, 0.51465034)\n",
      "(857, 0.5138935)\n",
      "(673, 0.5133911)\n",
      "(312, 0.51334435)\n",
      "(173, 0.5124544)\n",
      "(999, 0.5124071)\n",
      "(1000, 0.5120631)\n",
      "(26, 0.5118348)\n",
      "(124, 0.5113774)\n",
      "(625, 0.5110618)\n",
      "(758, 0.5105729)\n",
      "(882, 0.5105416)\n",
      "(431, 0.50966114)\n",
      "(263, 0.50936174)\n",
      "(948, 0.5093143)\n",
      "(265, 0.5092438)\n",
      "(627, 0.5091164)\n",
      "(162, 0.50896364)\n",
      "(984, 0.5089258)\n",
      "(901, 0.50882715)\n",
      "(687, 0.50861704)\n",
      "(466, 0.50859207)\n",
      "(917, 0.5080993)\n",
      "(672, 0.5080323)\n",
      "(299, 0.5079093)\n",
      "(579, 0.50785863)\n",
      "(583, 0.507851)\n",
      "(874, 0.5076206)\n",
      "(572, 0.5076138)\n",
      "(650, 0.50742567)\n",
      "(540, 0.5072701)\n",
      "(680, 0.507149)\n",
      "(73, 0.50707)\n",
      "(921, 0.507064)\n",
      "(322, 0.5070571)\n",
      "(370, 0.5068327)\n",
      "(898, 0.50675595)\n",
      "(341, 0.50671667)\n",
      "(703, 0.5066625)\n",
      "(636, 0.5063003)\n",
      "(521, 0.5062324)\n",
      "(304, 0.506191)\n",
      "(51, 0.5061626)\n",
      "(738, 0.50613374)\n",
      "(362, 0.5061017)\n",
      "(682, 0.50606567)\n",
      "(42, 0.50606316)\n",
      "(435, 0.505795)\n",
      "(205, 0.50578153)\n",
      "(407, 0.5056206)\n",
      "(836, 0.5054191)\n",
      "(6, 0.50507736)\n",
      "(969, 0.50486296)\n",
      "(114, 0.5048311)\n",
      "(841, 0.504721)\n",
      "(850, 0.5046002)\n",
      "(21, 0.50435174)\n",
      "(337, 0.50421005)\n",
      "(487, 0.50414914)\n",
      "(979, 0.5041083)\n",
      "(67, 0.5040621)\n",
      "(237, 0.5038075)\n",
      "(468, 0.5036979)\n",
      "(536, 0.50339603)\n",
      "(805, 0.5033959)\n",
      "(735, 0.50306755)\n",
      "(49, 0.5027023)\n",
      "(301, 0.502636)\n",
      "(190, 0.50259703)\n",
      "(106, 0.50254476)\n",
      "(267, 0.5024267)\n",
      "(613, 0.5023332)\n",
      "(918, 0.5023315)\n",
      "(108, 0.50225073)\n",
      "(793, 0.5022442)\n",
      "(1010, 0.50219405)\n",
      "(888, 0.5020105)\n",
      "(187, 0.5019872)\n",
      "(65, 0.5019573)\n",
      "(905, 0.5019298)\n",
      "(776, 0.50179106)\n",
      "(340, 0.501755)\n",
      "(401, 0.50171345)\n",
      "(320, 0.5016188)\n",
      "(679, 0.50160277)\n",
      "(657, 0.5014649)\n",
      "(520, 0.50142175)\n",
      "(820, 0.50141007)\n",
      "(234, 0.5012938)\n",
      "(669, 0.5012045)\n",
      "(154, 0.50119036)\n",
      "(656, 0.5011357)\n",
      "(118, 0.50112677)\n",
      "(38, 0.5010341)\n",
      "(870, 0.5009586)\n",
      "(128, 0.5009315)\n",
      "(90, 0.500926)\n",
      "(995, 0.50077426)\n",
      "(560, 0.5007291)\n",
      "(516, 0.5006937)\n",
      "(164, 0.50059944)\n",
      "(14, 0.5005002)\n",
      "(772, 0.500461)\n",
      "(245, 0.50032675)\n",
      "(39, 0.5003055)\n",
      "(318, 0.5002415)\n",
      "(582, 0.5001402)\n",
      "(168, 0.5000018)\n",
      "(617, 0.4999977)\n",
      "(759, 0.49992397)\n",
      "(862, 0.499863)\n",
      "(428, 0.499483)\n",
      "(786, 0.49944788)\n",
      "(321, 0.49943003)\n",
      "(849, 0.4994297)\n",
      "(877, 0.4993997)\n",
      "(467, 0.49939737)\n",
      "(485, 0.49939328)\n",
      "(235, 0.4993415)\n",
      "(439, 0.49933696)\n",
      "(241, 0.4992493)\n",
      "(295, 0.4991973)\n",
      "(136, 0.49917147)\n",
      "(56, 0.49916306)\n",
      "(621, 0.49913856)\n",
      "(808, 0.49913538)\n",
      "(412, 0.4990124)\n",
      "(612, 0.49900755)\n",
      "(794, 0.49894696)\n",
      "(76, 0.49889007)\n",
      "(286, 0.49887946)\n",
      "(590, 0.49887374)\n",
      "(311, 0.498817)\n",
      "(1, 0.498703)\n",
      "(229, 0.49866608)\n",
      "(149, 0.49856555)\n",
      "(356, 0.49855644)\n",
      "(748, 0.49854103)\n",
      "(783, 0.49849164)\n",
      "(711, 0.49847245)\n",
      "(604, 0.49844855)\n",
      "(170, 0.49839014)\n",
      "(899, 0.49834937)\n",
      "(323, 0.49810708)\n",
      "(101, 0.49807614)\n",
      "(677, 0.4980276)\n",
      "(255, 0.49796322)\n",
      "(262, 0.49792737)\n",
      "(34, 0.49791077)\n",
      "(94, 0.49787256)\n",
      "(425, 0.4978667)\n",
      "(277, 0.4978544)\n",
      "(131, 0.49782842)\n",
      "(539, 0.49779373)\n",
      "(780, 0.49777916)\n",
      "(249, 0.49775812)\n",
      "(491, 0.4977332)\n",
      "(12, 0.49768665)\n",
      "(497, 0.4976159)\n",
      "(423, 0.49753776)\n",
      "(733, 0.49752527)\n",
      "(844, 0.49748042)\n",
      "(597, 0.4973568)\n",
      "(261, 0.49733126)\n",
      "(1005, 0.4972882)\n",
      "(950, 0.49725577)\n",
      "(266, 0.49719125)\n",
      "(4, 0.49713957)\n",
      "(796, 0.49712938)\n",
      "(865, 0.4971042)\n",
      "(585, 0.49709728)\n",
      "(912, 0.49708417)\n",
      "(84, 0.4970675)\n",
      "(85, 0.49706346)\n",
      "(550, 0.49699664)\n",
      "(488, 0.49697176)\n",
      "(553, 0.49695888)\n",
      "(384, 0.49695322)\n",
      "(354, 0.4968253)\n",
      "(891, 0.49677324)\n",
      "(185, 0.4967305)\n",
      "(694, 0.49670196)\n",
      "(74, 0.49668187)\n",
      "(479, 0.49661756)\n",
      "(953, 0.49660897)\n",
      "(144, 0.49657276)\n",
      "(977, 0.4965549)\n",
      "(275, 0.49653915)\n",
      "(433, 0.49648196)\n",
      "(135, 0.4964273)\n",
      "(308, 0.49642298)\n",
      "(607, 0.4963451)\n",
      "(634, 0.49624458)\n",
      "(373, 0.49621338)\n",
      "(157, 0.496192)\n",
      "(156, 0.49617907)\n",
      "(448, 0.49608415)\n",
      "(523, 0.49607664)\n",
      "(451, 0.4960488)\n",
      "(551, 0.49600062)\n",
      "(220, 0.49598107)\n",
      "(537, 0.4959345)\n",
      "(869, 0.49590388)\n",
      "(406, 0.4958712)\n",
      "(342, 0.49581414)\n",
      "(761, 0.49574265)\n",
      "(742, 0.49568892)\n",
      "(360, 0.4956795)\n",
      "(383, 0.49562386)\n",
      "(692, 0.49553135)\n",
      "(27, 0.49551055)\n",
      "(695, 0.49549526)\n",
      "(53, 0.49549514)\n",
      "(222, 0.49549285)\n",
      "(941, 0.49549237)\n",
      "(514, 0.4953887)\n",
      "(517, 0.4953826)\n",
      "(418, 0.49536923)\n",
      "(623, 0.49535367)\n",
      "(1017, 0.4952923)\n",
      "(443, 0.49528837)\n",
      "(817, 0.49524724)\n",
      "(142, 0.49523777)\n",
      "(16, 0.49521887)\n",
      "(186, 0.49509898)\n",
      "(398, 0.49509165)\n",
      "(541, 0.49506566)\n",
      "(282, 0.49505836)\n",
      "(690, 0.49505016)\n",
      "(741, 0.49503195)\n",
      "(474, 0.49502435)\n",
      "(954, 0.49497443)\n",
      "(732, 0.494951)\n",
      "(967, 0.49490702)\n",
      "(86, 0.49490067)\n",
      "(386, 0.4948786)\n",
      "(246, 0.49477363)\n",
      "(802, 0.49470186)\n",
      "(608, 0.49466848)\n",
      "(1018, 0.49463037)\n",
      "(611, 0.49461758)\n",
      "(116, 0.49456817)\n",
      "(729, 0.49452183)\n",
      "(816, 0.49446514)\n",
      "(724, 0.4944422)\n",
      "(777, 0.49443635)\n",
      "(292, 0.49437875)\n",
      "(213, 0.49429297)\n",
      "(111, 0.4942694)\n",
      "(767, 0.49418682)\n",
      "(64, 0.49415997)\n",
      "(668, 0.4941136)\n",
      "(764, 0.49410728)\n",
      "(29, 0.49404997)\n",
      "(851, 0.49402952)\n",
      "(48, 0.49398875)\n",
      "(225, 0.49392232)\n",
      "(446, 0.49388626)\n",
      "(504, 0.49380943)\n",
      "(327, 0.4937958)\n",
      "(212, 0.49371868)\n",
      "(465, 0.493639)\n",
      "(885, 0.49362093)\n",
      "(382, 0.49359187)\n",
      "(631, 0.49355468)\n",
      "(363, 0.49353805)\n",
      "(822, 0.4934894)\n",
      "(980, 0.49346003)\n",
      "(259, 0.49345025)\n",
      "(704, 0.49344903)\n",
      "(757, 0.49344656)\n",
      "(215, 0.49325353)\n",
      "(203, 0.49323362)\n",
      "(9, 0.4931932)\n",
      "(745, 0.49313194)\n",
      "(441, 0.49309647)\n",
      "(161, 0.4930838)\n",
      "(788, 0.49307027)\n",
      "(82, 0.4930593)\n",
      "(104, 0.4929835)\n",
      "(538, 0.49296147)\n",
      "(57, 0.49293253)\n",
      "(892, 0.4929228)\n",
      "(75, 0.4928477)\n",
      "(498, 0.49276054)\n",
      "(264, 0.4927424)\n",
      "(666, 0.4927128)\n",
      "(177, 0.4926975)\n",
      "(653, 0.49267703)\n",
      "(570, 0.49266252)\n",
      "(405, 0.4926444)\n",
      "(791, 0.49248236)\n",
      "(648, 0.4924634)\n",
      "(359, 0.4924461)\n",
      "(357, 0.49241942)\n",
      "(971, 0.4923651)\n",
      "(438, 0.49229705)\n",
      "(790, 0.49221542)\n",
      "(829, 0.49219674)\n",
      "(603, 0.49216735)\n",
      "(437, 0.49216372)\n",
      "(10, 0.49212286)\n",
      "(194, 0.4921218)\n",
      "(991, 0.49207896)\n",
      "(2, 0.4920709)\n",
      "(238, 0.4920215)\n",
      "(1020, 0.49195752)\n",
      "(853, 0.49194533)\n",
      "(100, 0.49193934)\n",
      "(121, 0.49191993)\n",
      "(387, 0.49178356)\n",
      "(273, 0.49170548)\n",
      "(860, 0.49168462)\n",
      "(388, 0.49166155)\n",
      "(566, 0.49164262)\n",
      "(211, 0.49162045)\n",
      "(472, 0.49156237)\n",
      "(678, 0.49155518)\n",
      "(132, 0.49155146)\n",
      "(453, 0.49153593)\n",
      "(169, 0.49148697)\n",
      "(970, 0.4913448)\n",
      "(298, 0.49133086)\n",
      "(141, 0.49132425)\n",
      "(91, 0.4913064)\n",
      "(775, 0.4912771)\n",
      "(495, 0.49127343)\n",
      "(236, 0.4912459)\n",
      "(432, 0.49119005)\n",
      "(20, 0.49112478)\n",
      "(447, 0.49110806)\n",
      "(945, 0.49109027)\n",
      "(254, 0.49105483)\n",
      "(578, 0.49099597)\n",
      "(219, 0.49098766)\n",
      "(224, 0.49089903)\n",
      "(88, 0.49088797)\n",
      "(288, 0.49085107)\n",
      "(944, 0.49084964)\n",
      "(35, 0.49083507)\n",
      "(316, 0.49077615)\n",
      "(861, 0.4907694)\n",
      "(178, 0.49076506)\n",
      "(893, 0.49075362)\n",
      "(208, 0.49070725)\n",
      "(138, 0.4906987)\n",
      "(883, 0.4906769)\n",
      "(937, 0.4906681)\n",
      "(66, 0.49065128)\n",
      "(409, 0.4906229)\n",
      "(924, 0.4905772)\n",
      "(712, 0.49057636)\n",
      "(361, 0.49055514)\n",
      "(332, 0.4905478)\n",
      "(251, 0.49050966)\n",
      "(721, 0.4905019)\n",
      "(372, 0.4904927)\n",
      "(616, 0.49046123)\n",
      "(789, 0.49041227)\n",
      "(531, 0.49040282)\n",
      "(887, 0.4903852)\n",
      "(947, 0.49037763)\n",
      "(399, 0.49031952)\n",
      "(557, 0.4903121)\n",
      "(897, 0.490287)\n",
      "(191, 0.49025488)\n",
      "(856, 0.4902532)\n",
      "(367, 0.4902405)\n",
      "(722, 0.49023476)\n",
      "(654, 0.4901437)\n",
      "(739, 0.49011803)\n",
      "(686, 0.49001896)\n",
      "(527, 0.49001488)\n",
      "(1002, 0.48999727)\n",
      "(319, 0.4899824)\n",
      "(744, 0.489958)\n",
      "(139, 0.48995367)\n",
      "(871, 0.4898448)\n",
      "(880, 0.4898397)\n",
      "(204, 0.48983517)\n",
      "(352, 0.4897839)\n",
      "(420, 0.48976612)\n",
      "(378, 0.48974168)\n",
      "(115, 0.48971707)\n",
      "(622, 0.4897145)\n",
      "(951, 0.489708)\n",
      "(206, 0.48967928)\n",
      "(810, 0.48967907)\n",
      "(558, 0.48965696)\n",
      "(507, 0.48958078)\n",
      "(480, 0.48957157)\n",
      "(281, 0.48955882)\n",
      "(137, 0.4895318)\n",
      "(713, 0.48949486)\n",
      "(807, 0.4894813)\n",
      "(518, 0.48942637)\n",
      "(315, 0.48940593)\n",
      "(103, 0.48932138)\n",
      "(647, 0.48920146)\n",
      "(112, 0.48917326)\n",
      "(602, 0.48916024)\n",
      "(556, 0.48910427)\n",
      "(642, 0.48905417)\n",
      "(532, 0.48899728)\n",
      "(120, 0.4889287)\n",
      "(15, 0.4889258)\n",
      "(819, 0.48884818)\n",
      "(233, 0.48884806)\n",
      "(92, 0.4887631)\n",
      "(8, 0.48875716)\n",
      "(752, 0.48874742)\n",
      "(1003, 0.48871753)\n",
      "(873, 0.48869982)\n",
      "(416, 0.4886448)\n",
      "(683, 0.4886416)\n",
      "(601, 0.48863128)\n",
      "(502, 0.48862386)\n",
      "(876, 0.48850608)\n",
      "(529, 0.48841098)\n",
      "(470, 0.48841083)\n",
      "(981, 0.48839408)\n",
      "(895, 0.48838973)\n",
      "(705, 0.48837256)\n",
      "(638, 0.48835763)\n",
      "(148, 0.48834795)\n",
      "(884, 0.48833767)\n",
      "(576, 0.48830178)\n",
      "(768, 0.48827067)\n",
      "(444, 0.48824465)\n",
      "(41, 0.48822752)\n",
      "(522, 0.4882177)\n",
      "(959, 0.4882127)\n",
      "(257, 0.48820412)\n",
      "(708, 0.48818928)\n",
      "(463, 0.4881735)\n",
      "(586, 0.48817128)\n",
      "(1004, 0.48813167)\n",
      "(50, 0.48812354)\n",
      "(938, 0.48810223)\n",
      "(280, 0.48799226)\n",
      "(274, 0.48798075)\n",
      "(813, 0.48797908)\n",
      "(252, 0.4879521)\n",
      "(45, 0.48791352)\n",
      "(348, 0.4879078)\n",
      "(105, 0.4878853)\n",
      "(552, 0.48785904)\n",
      "(728, 0.48784298)\n",
      "(375, 0.48783827)\n",
      "(587, 0.4878272)\n",
      "(717, 0.48779228)\n",
      "(253, 0.48779106)\n",
      "(928, 0.48777032)\n",
      "(303, 0.48776942)\n",
      "(508, 0.48774913)\n",
      "(643, 0.48773357)\n",
      "(256, 0.4876695)\n",
      "(419, 0.48766184)\n",
      "(333, 0.48764428)\n",
      "(469, 0.48760784)\n",
      "(344, 0.48760673)\n",
      "(151, 0.48760575)\n",
      "(763, 0.48760024)\n",
      "(513, 0.4875864)\n",
      "(61, 0.4875759)\n",
      "(195, 0.48753214)\n",
      "(335, 0.48746306)\n",
      "(501, 0.48745698)\n",
      "(226, 0.48744643)\n",
      "(993, 0.4873843)\n",
      "(943, 0.4873379)\n",
      "(674, 0.4872703)\n",
      "(461, 0.4871834)\n",
      "(842, 0.4871719)\n",
      "(676, 0.48713958)\n",
      "(349, 0.4871057)\n",
      "(598, 0.48704237)\n",
      "(192, 0.48696998)\n",
      "(276, 0.4869249)\n",
      "(279, 0.4869219)\n",
      "(801, 0.4869)\n",
      "(484, 0.48686153)\n",
      "(287, 0.48684844)\n",
      "(770, 0.48683614)\n",
      "(494, 0.48683238)\n",
      "(867, 0.48681375)\n",
      "(96, 0.48678866)\n",
      "(949, 0.4866935)\n",
      "(756, 0.4866803)\n",
      "(818, 0.48667562)\n",
      "(434, 0.48665184)\n",
      "(376, 0.48665044)\n",
      "(159, 0.48659986)\n",
      "(228, 0.4865804)\n",
      "(458, 0.48656267)\n",
      "(422, 0.48649704)\n",
      "(328, 0.48645115)\n",
      "(809, 0.48644692)\n",
      "(331, 0.4864355)\n",
      "(624, 0.48642218)\n",
      "(430, 0.48641714)\n",
      "(404, 0.4864031)\n",
      "(109, 0.48639202)\n",
      "(365, 0.48638245)\n",
      "(942, 0.48633105)\n",
      "(827, 0.48631904)\n",
      "(779, 0.48631027)\n",
      "(1012, 0.4862958)\n",
      "(268, 0.48628446)\n",
      "(134, 0.48624867)\n",
      "(985, 0.48624095)\n",
      "(415, 0.48615578)\n",
      "(394, 0.48610365)\n",
      "(544, 0.48606098)\n",
      "(343, 0.4860555)\n",
      "(635, 0.48602146)\n",
      "(771, 0.48597392)\n",
      "(270, 0.4859721)\n",
      "(271, 0.48597106)\n",
      "(364, 0.48588854)\n",
      "(18, 0.48586175)\n",
      "(1007, 0.48580632)\n",
      "(913, 0.48568383)\n",
      "(670, 0.4856694)\n",
      "(968, 0.48562175)\n",
      "(403, 0.48558867)\n",
      "(133, 0.48557872)\n",
      "(297, 0.48554438)\n",
      "(172, 0.4855403)\n",
      "(618, 0.48553243)\n",
      "(785, 0.4855244)\n",
      "(140, 0.48549902)\n",
      "(511, 0.4854778)\n",
      "(1019, 0.48542085)\n",
      "(239, 0.4853351)\n",
      "(63, 0.48531914)\n",
      "(649, 0.48530266)\n",
      "(545, 0.4852875)\n",
      "(935, 0.48528025)\n",
      "(473, 0.4852474)\n",
      "(691, 0.48523933)\n",
      "(528, 0.48519534)\n",
      "(290, 0.48517781)\n",
      "(754, 0.48516092)\n",
      "(478, 0.4851512)\n",
      "(244, 0.48507708)\n",
      "(929, 0.4850729)\n",
      "(325, 0.48502874)\n",
      "(7, 0.4850086)\n",
      "(664, 0.48495224)\n",
      "(996, 0.48492077)\n",
      "(615, 0.48484018)\n",
      "(639, 0.48478785)\n",
      "(919, 0.48478538)\n",
      "(872, 0.48460308)\n",
      "(571, 0.48459548)\n",
      "(28, 0.48458102)\n",
      "(811, 0.4845782)\n",
      "(338, 0.48456675)\n",
      "(218, 0.48454595)\n",
      "(427, 0.48454055)\n",
      "(955, 0.48453957)\n",
      "(546, 0.4845348)\n",
      "(886, 0.48452967)\n",
      "(182, 0.4844818)\n",
      "(83, 0.48448017)\n",
      "(681, 0.48440847)\n",
      "(824, 0.48432502)\n",
      "(130, 0.4843115)\n",
      "(826, 0.48428944)\n",
      "(600, 0.48428383)\n",
      "(199, 0.48425126)\n",
      "(812, 0.48420534)\n",
      "(493, 0.4841989)\n",
      "(696, 0.48413447)\n",
      "(743, 0.48404917)\n",
      "(313, 0.48402363)\n",
      "(830, 0.4840146)\n",
      "(519, 0.4839581)\n",
      "(165, 0.48392197)\n",
      "(250, 0.48392102)\n",
      "(503, 0.483895)\n",
      "(554, 0.4838778)\n",
      "(592, 0.4838465)\n",
      "(402, 0.48382497)\n",
      "(661, 0.48375186)\n",
      "(97, 0.4837246)\n",
      "(881, 0.4835776)\n",
      "(685, 0.4835637)\n",
      "(176, 0.48354027)\n",
      "(835, 0.48351526)\n",
      "(505, 0.48346105)\n",
      "(660, 0.48345524)\n",
      "(957, 0.48344603)\n",
      "(33, 0.48335475)\n",
      "(47, 0.48335335)\n",
      "(931, 0.4833441)\n",
      "(894, 0.4833389)\n",
      "(581, 0.48323825)\n",
      "(974, 0.4832329)\n",
      "(217, 0.4831626)\n",
      "(900, 0.48312742)\n",
      "(329, 0.48311788)\n",
      "(289, 0.48311418)\n",
      "(561, 0.4830755)\n",
      "(11, 0.48301083)\n",
      "(542, 0.4829466)\n",
      "(368, 0.48294583)\n",
      "(62, 0.48293936)\n",
      "(987, 0.48288694)\n",
      "(214, 0.48287728)\n",
      "(179, 0.48287106)\n",
      "(535, 0.4828279)\n",
      "(167, 0.4828234)\n",
      "(840, 0.48279428)\n",
      "(814, 0.4827843)\n",
      "(896, 0.48276457)\n",
      "(889, 0.4827001)\n",
      "(990, 0.4826878)\n",
      "(294, 0.48263353)\n",
      "(95, 0.48262152)\n",
      "(795, 0.4826115)\n",
      "(216, 0.48260793)\n",
      "(210, 0.48247904)\n",
      "(591, 0.48247054)\n",
      "(326, 0.48246568)\n",
      "(223, 0.48238933)\n",
      "(232, 0.4823848)\n",
      "(564, 0.48237273)\n",
      "(569, 0.48234668)\n",
      "(821, 0.48234186)\n",
      "(831, 0.48232463)\n",
      "(675, 0.48231912)\n",
      "(973, 0.4822638)\n",
      "(956, 0.48224497)\n",
      "(599, 0.48222053)\n",
      "(80, 0.48221394)\n",
      "(374, 0.4822131)\n",
      "(645, 0.48220614)\n",
      "(17, 0.48214749)\n",
      "(209, 0.4821089)\n",
      "(574, 0.4820944)\n",
      "(630, 0.4820853)\n",
      "(300, 0.48204172)\n",
      "(417, 0.48200485)\n",
      "(127, 0.48199898)\n",
      "(421, 0.48198497)\n",
      "(202, 0.48194426)\n",
      "(293, 0.48189914)\n",
      "(411, 0.4818623)\n",
      "(988, 0.48180953)\n",
      "(629, 0.481794)\n",
      "(163, 0.48179045)\n",
      "(291, 0.48178536)\n",
      "(181, 0.48170024)\n",
      "(445, 0.48169023)\n",
      "(283, 0.4816787)\n",
      "(641, 0.48164958)\n",
      "(107, 0.48160988)\n",
      "(1009, 0.48159564)\n",
      "(746, 0.48153174)\n",
      "(183, 0.48151365)\n",
      "(914, 0.48150387)\n",
      "(260, 0.4814875)\n",
      "(1011, 0.48145458)\n",
      "(737, 0.48142552)\n",
      "(997, 0.48140934)\n",
      "(429, 0.48138073)\n",
      "(976, 0.4813771)\n",
      "(646, 0.48134416)\n",
      "(845, 0.48126522)\n",
      "(314, 0.48125848)\n",
      "(707, 0.48122647)\n",
      "(371, 0.4812261)\n",
      "(200, 0.4811329)\n",
      "(389, 0.48112527)\n",
      "(242, 0.48112404)\n",
      "(476, 0.481095)\n",
      "(839, 0.48108518)\n",
      "(483, 0.48099223)\n",
      "(543, 0.48087254)\n",
      "(986, 0.48083806)\n",
      "(79, 0.4808073)\n",
      "(3, 0.48079923)\n",
      "(1015, 0.4807022)\n",
      "(864, 0.48069385)\n",
      "(462, 0.4806064)\n",
      "(524, 0.4805423)\n",
      "(749, 0.4805093)\n",
      "(594, 0.48044777)\n",
      "(916, 0.48039293)\n",
      "(846, 0.48037705)\n",
      "(102, 0.48033723)\n",
      "(414, 0.4803216)\n",
      "(559, 0.48027292)\n",
      "(146, 0.4802603)\n",
      "(720, 0.48020503)\n",
      "(858, 0.48019257)\n",
      "(60, 0.48015687)\n",
      "(908, 0.48014146)\n",
      "(589, 0.48011371)\n",
      "(833, 0.48011267)\n",
      "(701, 0.48008868)\n",
      "(698, 0.48008105)\n",
      "(903, 0.48007756)\n",
      "(379, 0.4800773)\n",
      "(87, 0.48006082)\n",
      "(549, 0.48000893)\n",
      "(652, 0.47995746)\n",
      "(725, 0.47992092)\n",
      "(1014, 0.47991133)\n",
      "(24, 0.47988015)\n",
      "(781, 0.4798086)\n",
      "(765, 0.4797919)\n",
      "(706, 0.4797461)\n",
      "(584, 0.47972676)\n",
      "(920, 0.47971752)\n",
      "(193, 0.47969753)\n",
      "(143, 0.47969523)\n",
      "(475, 0.47967756)\n",
      "(89, 0.47964066)\n",
      "(859, 0.47963908)\n",
      "(688, 0.47962525)\n",
      "(129, 0.47960955)\n",
      "(662, 0.4795156)\n",
      "(906, 0.47948086)\n",
      "(828, 0.479462)\n",
      "(878, 0.47941822)\n",
      "(644, 0.47936404)\n",
      "(390, 0.47932705)\n",
      "(1006, 0.47930905)\n",
      "(689, 0.4793079)\n",
      "(345, 0.47929725)\n",
      "(731, 0.47919792)\n",
      "(588, 0.47916758)\n",
      "(23, 0.4791509)\n",
      "(605, 0.4791357)\n",
      "(52, 0.47911787)\n",
      "(72, 0.47908252)\n",
      "(380, 0.4790403)\n",
      "(927, 0.4790208)\n",
      "(117, 0.47900015)\n",
      "(577, 0.47897974)\n",
      "(930, 0.47897086)\n",
      "(227, 0.4789698)\n",
      "(933, 0.4788954)\n",
      "(640, 0.478858)\n",
      "(715, 0.47882813)\n",
      "(952, 0.4788259)\n",
      "(459, 0.47879472)\n",
      "(798, 0.4787697)\n",
      "(925, 0.4787461)\n",
      "(609, 0.47871864)\n",
      "(396, 0.4786913)\n",
      "(413, 0.47865993)\n",
      "(346, 0.47860312)\n",
      "(71, 0.47859868)\n",
      "(530, 0.47859022)\n",
      "(150, 0.47856644)\n",
      "(926, 0.47856206)\n",
      "(751, 0.47854882)\n",
      "(548, 0.4785171)\n",
      "(489, 0.47848064)\n",
      "(353, 0.47840846)\n",
      "(366, 0.47830534)\n",
      "(610, 0.47816417)\n",
      "(832, 0.47816136)\n",
      "(1021, 0.47811827)\n",
      "(726, 0.47789395)\n",
      "(68, 0.47788867)\n",
      "(481, 0.47774774)\n",
      "(902, 0.47769108)\n",
      "(911, 0.4776414)\n",
      "(699, 0.47762716)\n",
      "(145, 0.47753772)\n",
      "(632, 0.47751105)\n",
      "(496, 0.47750297)\n",
      "(201, 0.47748458)\n",
      "(449, 0.47746363)\n",
      "(306, 0.47742513)\n",
      "(792, 0.47741735)\n",
      "(854, 0.47732574)\n",
      "(477, 0.47729665)\n",
      "(44, 0.4772721)\n",
      "(534, 0.47721878)\n",
      "(958, 0.4771962)\n",
      "(803, 0.4771819)\n",
      "(25, 0.47712848)\n",
      "(125, 0.4771171)\n",
      "(351, 0.47707415)\n",
      "(500, 0.47706905)\n",
      "(153, 0.47694772)\n",
      "(269, 0.47687876)\n",
      "(863, 0.47684744)\n",
      "(606, 0.47683352)\n",
      "(158, 0.47677466)\n",
      "(336, 0.47669545)\n",
      "(324, 0.47668624)\n",
      "(1016, 0.476669)\n",
      "(998, 0.47666368)\n",
      "(188, 0.4766538)\n",
      "(626, 0.47662035)\n",
      "(198, 0.47661674)\n",
      "(381, 0.47658265)\n",
      "(457, 0.47649693)\n",
      "(330, 0.47649005)\n",
      "(152, 0.47607243)\n",
      "(734, 0.47606108)\n",
      "(355, 0.47590548)\n",
      "(964, 0.4758878)\n",
      "(231, 0.47587696)\n",
      "(972, 0.4757401)\n",
      "(310, 0.4756624)\n",
      "(563, 0.47563338)\n",
      "(307, 0.47553056)\n",
      "(838, 0.4754808)\n",
      "(175, 0.47546998)\n",
      "(526, 0.4754683)\n",
      "(975, 0.47546345)\n",
      "(909, 0.47540617)\n",
      "(562, 0.47531524)\n",
      "(966, 0.47530103)\n",
      "(723, 0.4752858)\n",
      "(486, 0.4752854)\n",
      "(755, 0.47525507)\n",
      "(614, 0.47521698)\n",
      "(450, 0.475134)\n",
      "(800, 0.4751286)\n",
      "(155, 0.47496688)\n",
      "(166, 0.4749391)\n",
      "(797, 0.4748441)\n",
      "(651, 0.47479638)\n",
      "(515, 0.47479424)\n",
      "(305, 0.47471455)\n",
      "(932, 0.4744627)\n",
      "(978, 0.47445476)\n",
      "(302, 0.47440818)\n",
      "(568, 0.47430047)\n",
      "(69, 0.47425985)\n",
      "(98, 0.47410214)\n",
      "(595, 0.4741016)\n",
      "(180, 0.47409528)\n",
      "(334, 0.474078)\n",
      "(910, 0.47387043)\n",
      "(197, 0.47380534)\n",
      "(347, 0.47379595)\n",
      "(567, 0.47375792)\n",
      "(855, 0.4736764)\n",
      "(400, 0.47363606)\n",
      "(740, 0.4735519)\n",
      "(565, 0.47352606)\n",
      "(750, 0.47348496)\n",
      "(658, 0.4734241)\n",
      "(243, 0.47342363)\n",
      "(659, 0.4733938)\n",
      "(5, 0.4733694)\n",
      "(171, 0.47334707)\n",
      "(923, 0.47329643)\n",
      "(736, 0.47327644)\n",
      "(391, 0.47312862)\n",
      "(710, 0.47308984)\n",
      "(424, 0.4730834)\n",
      "(575, 0.47306314)\n",
      "(395, 0.47291508)\n",
      "(93, 0.47276515)\n",
      "(946, 0.47256827)\n",
      "(693, 0.47243032)\n",
      "(240, 0.47242713)\n",
      "(46, 0.472395)\n",
      "(702, 0.47218293)\n",
      "(848, 0.47217968)\n",
      "(13, 0.4721719)\n",
      "(350, 0.47206238)\n",
      "(573, 0.47201288)\n",
      "(70, 0.47195297)\n",
      "(994, 0.47190067)\n",
      "(667, 0.47188723)\n",
      "(547, 0.47187164)\n",
      "(825, 0.47153974)\n",
      "(769, 0.47152776)\n",
      "(718, 0.47151262)\n",
      "(939, 0.4713892)\n",
      "(782, 0.47125098)\n",
      "(684, 0.47119835)\n",
      "(499, 0.4711561)\n",
      "(580, 0.4710635)\n",
      "(915, 0.4710033)\n",
      "(77, 0.47098446)\n",
      "(804, 0.470901)\n",
      "(174, 0.47089267)\n",
      "(823, 0.47072595)\n",
      "(778, 0.47061902)\n",
      "(992, 0.47056144)\n",
      "(904, 0.4705457)\n",
      "(907, 0.4703569)\n",
      "(1008, 0.47023246)\n",
      "(58, 0.47008145)\n",
      "(762, 0.47006294)\n",
      "(184, 0.4700255)\n",
      "(284, 0.47001067)\n",
      "(442, 0.46996945)\n",
      "(773, 0.46978825)\n",
      "(815, 0.46965277)\n",
      "(843, 0.46953803)\n",
      "(207, 0.46952075)\n",
      "(753, 0.46925095)\n",
      "(806, 0.46919155)\n",
      "(847, 0.46916243)\n",
      "(879, 0.4690232)\n",
      "(110, 0.46893823)\n",
      "(189, 0.46889037)\n",
      "(309, 0.4688317)\n",
      "(78, 0.46875682)\n",
      "(32, 0.46868983)\n",
      "(40, 0.46846667)\n",
      "(296, 0.46844292)\n",
      "(709, 0.4684143)\n",
      "(700, 0.46840364)\n",
      "(285, 0.46833736)\n",
      "(455, 0.46827644)\n",
      "(147, 0.4682562)\n",
      "(248, 0.4680766)\n",
      "(787, 0.4679756)\n",
      "(440, 0.46791333)\n",
      "(936, 0.467628)\n",
      "(525, 0.46729028)\n",
      "(119, 0.46726555)\n",
      "(464, 0.46722943)\n",
      "(940, 0.4672133)\n",
      "(126, 0.4669931)\n",
      "(31, 0.46693295)\n",
      "(963, 0.4667221)\n",
      "(19, 0.4666685)\n",
      "(30, 0.46656227)\n",
      "(452, 0.4663192)\n",
      "(22, 0.4662401)\n",
      "(426, 0.46623996)\n",
      "(377, 0.46620438)\n",
      "(868, 0.4661462)\n",
      "(665, 0.46576223)\n",
      "(456, 0.46564624)\n",
      "(123, 0.46553734)\n",
      "(55, 0.46526352)\n",
      "(965, 0.46492016)\n",
      "(730, 0.46439183)\n",
      "(989, 0.4643184)\n",
      "(339, 0.46423116)\n",
      "(593, 0.4639503)\n",
      "(596, 0.4638316)\n",
      "(506, 0.46380818)\n",
      "(230, 0.4632481)\n",
      "(655, 0.4630413)\n",
      "(221, 0.46284938)\n",
      "(875, 0.46282235)\n",
      "(934, 0.4626473)\n",
      "(637, 0.4616867)\n",
      "(834, 0.4610101)\n",
      "(160, 0.4605283)\n",
      "(837, 0.4600808)\n",
      "(852, 0.4598908)\n",
      "(408, 0.45904025)\n",
      "(392, 0.4575454)\n",
      "(0, 0.4573209)\n",
      "(358, 0.45670915)\n",
      "(36, 0.45665094)\n",
      "(983, 0.4543359)\n",
      "(460, 0.45300815)\n",
      "(369, 0.43578908)\n"
     ]
    }
   ],
   "source": [
    "prob = model.get_gates(mode = 'prob')\n",
    "prob_filtered = []\n",
    "for i, x in enumerate(prob):\n",
    "    prob_filtered.append((i, x))\n",
    "\n",
    "    \n",
    "prob_filtered.sort(key = lambda x: x[1], reverse=True)\n",
    "print(len(prob_filtered))\n",
    "print(*prob_filtered, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "(43, 0.30065265)\n",
      "(385, 0.27911294)\n",
      "(766, 0.20166779)\n",
      "(760, 0.15845281)\n",
      "(397, 0.12267709)\n",
      "(317, 0.09799548)\n",
      "(512, 0.09525472)\n",
      "(436, 0.09455479)\n",
      "(509, 0.09013426)\n",
      "(633, 0.08866724)\n",
      "(774, 0.08656566)\n",
      "(890, 0.08398581)\n",
      "(716, 0.079407625)\n",
      "(410, 0.07811102)\n",
      "(799, 0.07642231)\n",
      "(1013, 0.07197354)\n",
      "(962, 0.071404494)\n",
      "(620, 0.053865112)\n",
      "(866, 0.04754703)\n",
      "(196, 0.044944543)\n",
      "(81, 0.044801995)\n",
      "(490, 0.040303055)\n",
      "(628, 0.03975086)\n",
      "(54, 0.036927465)\n",
      "(482, 0.036329765)\n",
      "(533, 0.03587929)\n",
      "(454, 0.034916062)\n",
      "(122, 0.03450165)\n",
      "(663, 0.031579692)\n",
      "(510, 0.02963584)\n",
      "(99, 0.029284444)\n",
      "(697, 0.028994502)\n",
      "(714, 0.028827349)\n",
      "(961, 0.024978412)\n",
      "(1001, 0.024607953)\n",
      "(471, 0.024387468)\n",
      "(960, 0.024247754)\n",
      "(272, 0.023226768)\n",
      "(982, 0.02299968)\n",
      "(278, 0.022513498)\n",
      "(671, 0.022444706)\n",
      "(727, 0.021970635)\n",
      "(619, 0.021576768)\n",
      "(922, 0.01955539)\n",
      "(258, 0.01938031)\n",
      "(247, 0.018537415)\n",
      "(555, 0.018296639)\n",
      "(784, 0.016355071)\n",
      "(747, 0.015629547)\n",
      "(37, 0.015445078)\n",
      "(393, 0.015388751)\n",
      "(492, 0.015287414)\n",
      "(719, 0.014722326)\n",
      "(113, 0.014658944)\n",
      "(59, 0.014650338)\n",
      "(857, 0.013893483)\n",
      "(673, 0.013391065)\n",
      "(312, 0.013344327)\n",
      "(173, 0.012454386)\n",
      "(999, 0.012407123)\n",
      "(1000, 0.012063074)\n",
      "(26, 0.01183478)\n",
      "(124, 0.011377401)\n",
      "(625, 0.011061799)\n",
      "(758, 0.010572881)\n",
      "(882, 0.010541626)\n",
      "(431, 0.0096611455)\n",
      "(263, 0.009361725)\n",
      "(948, 0.009314275)\n",
      "(265, 0.009243761)\n",
      "(627, 0.009116395)\n",
      "(162, 0.008963643)\n",
      "(984, 0.008925797)\n",
      "(901, 0.008827178)\n",
      "(687, 0.008617065)\n",
      "(466, 0.008592065)\n",
      "(917, 0.008099305)\n",
      "(672, 0.008032329)\n",
      "(299, 0.007909308)\n",
      "(579, 0.00785865)\n",
      "(583, 0.007851001)\n",
      "(874, 0.0076205493)\n",
      "(572, 0.007613762)\n",
      "(650, 0.0074256845)\n",
      "(540, 0.0072700763)\n",
      "(680, 0.007148965)\n",
      "(73, 0.0070699756)\n",
      "(921, 0.007063963)\n",
      "(322, 0.0070570502)\n",
      "(370, 0.0068327403)\n",
      "(898, 0.0067559727)\n",
      "(341, 0.006716691)\n",
      "(703, 0.006662481)\n",
      "(636, 0.00630025)\n",
      "(521, 0.00623238)\n",
      "(304, 0.0061910097)\n",
      "(51, 0.006162593)\n",
      "(738, 0.0061337277)\n",
      "(362, 0.006101703)\n",
      "(682, 0.0060656755)\n",
      "(42, 0.0060631414)\n",
      "(435, 0.005794985)\n",
      "(205, 0.0057815076)\n",
      "(407, 0.0056206067)\n",
      "(836, 0.005419094)\n",
      "(6, 0.00507738)\n",
      "(969, 0.0048629516)\n",
      "(114, 0.004831089)\n",
      "(841, 0.0047209663)\n",
      "(850, 0.004600235)\n",
      "(21, 0.004351706)\n",
      "(337, 0.004210038)\n",
      "(487, 0.0041491115)\n",
      "(979, 0.0041083186)\n",
      "(67, 0.0040621385)\n",
      "(237, 0.0038074746)\n",
      "(468, 0.0036978442)\n",
      "(536, 0.0033960175)\n",
      "(805, 0.0033959341)\n",
      "(735, 0.003067533)\n",
      "(49, 0.0027022837)\n",
      "(301, 0.002635988)\n",
      "(190, 0.0025970202)\n",
      "(106, 0.0025447793)\n",
      "(267, 0.0024266997)\n",
      "(613, 0.0023332248)\n",
      "(918, 0.0023315146)\n",
      "(108, 0.0022507315)\n",
      "(793, 0.0022441715)\n",
      "(1010, 0.0021940328)\n",
      "(888, 0.0020105245)\n",
      "(187, 0.0019872135)\n",
      "(65, 0.0019572766)\n",
      "(905, 0.0019298105)\n",
      "(776, 0.001791042)\n",
      "(340, 0.001755028)\n",
      "(401, 0.0017134671)\n",
      "(320, 0.0016188313)\n",
      "(679, 0.0016027584)\n",
      "(657, 0.0014648861)\n",
      "(520, 0.0014217698)\n",
      "(820, 0.0014100948)\n",
      "(234, 0.0012937951)\n",
      "(669, 0.0012045022)\n",
      "(154, 0.0011903584)\n",
      "(656, 0.0011357053)\n",
      "(118, 0.0011267395)\n",
      "(38, 0.0010340909)\n",
      "(870, 0.0009586261)\n",
      "(128, 0.0009314806)\n",
      "(90, 0.000925994)\n",
      "(995, 0.0007742794)\n",
      "(560, 0.00072905904)\n",
      "(516, 0.0006936632)\n",
      "(164, 0.0005994357)\n",
      "(14, 0.000500229)\n",
      "(772, 0.00046096244)\n",
      "(245, 0.0003267264)\n",
      "(39, 0.0003054732)\n",
      "(318, 0.00024154104)\n",
      "(582, 0.00014019653)\n",
      "(168, 1.8139592e-06)\n"
     ]
    }
   ],
   "source": [
    "raw = model.get_gates(mode = 'raw')\n",
    "raw_filtered = []\n",
    "for i, x in enumerate(raw):\n",
    "    if x>0: raw_filtered.append((i, x))\n",
    "\n",
    "raw_filtered.sort(key = lambda x: x[1], reverse=True)\n",
    "print(len(raw_filtered))\n",
    "print(*raw_filtered, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
